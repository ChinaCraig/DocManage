import os
import json
import time
import logging
from flask import Blueprint, request, jsonify
from sqlalchemy import or_, and_, desc
from app.models.document_models import DocumentNode, db
from app.services.vectorization.vector_service_adapter import VectorServiceAdapter
from app.services.llm import LLMService
# æ—§çš„MCPæœåŠ¡å·²ç§»é™¤
from config import Config
import asyncio

logger = logging.getLogger(__name__)

search_bp = Blueprint('search', __name__)

@search_bp.route('/create-folder', methods=['POST'])
def create_folder():
    """åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆMCPåŠŸèƒ½ï¼‰"""
    try:
        data = request.get_json()
        folder_name = data.get('name', 'æ–°æ–‡ä»¶å¤¹').strip()
        parent_id = data.get('parent_id')
        
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # åˆ›å»ºæ–‡ä»¶å¤¹
        from app.models import DocumentNode
        from app import db
        
        new_folder = DocumentNode(
            name=folder_name,
            type='folder',
            parent_id=parent_id,
            description=f"ç”±MCPå·¥å…·åˆ›å»ºçš„æ–‡ä»¶å¤¹: {folder_name}"
        )
        
        db.session.add(new_folder)
        db.session.commit()
        
        logger.info(f"MCPæˆåŠŸåˆ›å»ºæ–‡ä»¶å¤¹: {folder_name} (ID: {new_folder.id})")
        
        return jsonify({
            'success': True,
            'data': {
                'id': new_folder.id,
                'name': new_folder.name,
                'parent_id': new_folder.parent_id,
                'message': f"æˆåŠŸåˆ›å»ºæ–‡ä»¶å¤¹: {folder_name}"
            }
        })
        
    except Exception as e:
        logger.error(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@search_bp.route('/', methods=['POST'])
def semantic_search():
    """è¯­ä¹‰æœç´¢æ¥å£"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸ºç©º'
            }), 400
        
        query_text = data.get('query', '').strip()
        top_k = data.get('top_k', 10)
        enable_mcp = data.get('enable_mcp', False)
        
        if not query_text:
            return jsonify({
                'success': False,
                'error': 'æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º'
            }), 400
        
        llm_model = data.get('llm_model')  # è·å–LLMæ¨¡å‹å‚æ•°
        
        logger.info(f"æ¥æ”¶åˆ°è¯­ä¹‰æœç´¢è¯·æ±‚: {query_text}, top_k: {top_k}, enable_mcp: {enable_mcp}, llm_model: {llm_model}")
        
        # åˆå§‹åŒ–ç»“æœ
        search_results = []
        mcp_tool_results = []
        skip_search = False
        intent_analysis = None
        
        # ä½¿ç”¨ä¸“ç”¨çš„æ„å›¾è¯†åˆ«æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ„å›¾åˆ†æ
        if Config.ENABLE_INTENT_ANALYSIS and enable_mcp:
            try:
                from app.services.llm import LLMService
                # ä½¿ç”¨ä¸“ç”¨çš„æ„å›¾è¯†åˆ«æ¨¡å‹
                intent_llm_model = f"{Config.INTENT_ANALYSIS_LLM_PROVIDER}:{Config.INTENT_ANALYSIS_LLM_MODEL}"
                intent_analysis = LLMService.analyze_user_intent(query_text, intent_llm_model)
                logger.info(f"æ„å›¾åˆ†æç»“æœ: {intent_analysis}")
                
                # ä½¿ç”¨é…ç½®çš„ç½®ä¿¡åº¦é˜ˆå€¼
                confidence_threshold = Config.INTENT_ANALYSIS_CONFIDENCE_THRESHOLD
                intent_type = intent_analysis.get('intent_type', 'knowledge_search')
                confidence = intent_analysis.get('confidence', 0)
                
                # æ ¹æ®æ–°çš„æ„å›¾åˆ†ç±»å¤„ç†
                if intent_type == 'normal_chat' and confidence > confidence_threshold:
                    # æ™®é€šèŠå¤©ï¼šç›´æ¥LLMé—®ç­”
                    logger.info(f"æ‰§è¡Œæ™®é€šèŠå¤©æ“ä½œ: {query_text}")
                    skip_search = True
                    
                    try:
                        # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„LLMæ¨¡å‹è¿›è¡ŒèŠå¤©
                        chat_response = LLMService.generate_answer(
                            query=query_text,
                            context="",  # æ™®é€šèŠå¤©ä¸éœ€è¦æ–‡æ¡£ä¸Šä¸‹æ–‡
                            llm_model=llm_model or intent_llm_model,
                            style="conversational"
                        )
                        logger.info(f"æ™®é€šèŠå¤©å“åº”ç”Ÿæˆå®Œæˆ")
                        
                        # æ„å»ºèŠå¤©å“åº”æ•°æ®
                        response_data = {
                            'query': query_text,
                            'is_chat': True,
                            'chat_response': chat_response,
                            'search_type': 'normal_chat',
                            'intent_analysis': {
                                'intent_type': intent_analysis.get('intent_type'),
                                'confidence': intent_analysis.get('confidence'),
                                'action_type': intent_analysis.get('action_type'),
                                'reasoning': intent_analysis.get('reasoning'),
                                'used_llm': True,
                                'model': llm_model or intent_llm_model,
                                'prompt_source': intent_analysis.get('prompt_source', 'config_file'),
                                'confidence_threshold': confidence_threshold
                            }
                        }
                        
                        # è½¬æ¢ä¸ºæ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼
                        message_content = []
                        
                        # æ·»åŠ æ„å›¾è¯†åˆ«ç»“æœ
                        message_content.append({
                            "type": "text", 
                            "data": f"ğŸ’¬ æ™®é€šå¯¹è¯: {intent_analysis.get('reasoning', 'å·²è¯†åˆ«ä¸ºä¸€èˆ¬å¯¹è¯äº¤æµ')}"
                        })
                        
                        # æ·»åŠ èŠå¤©å“åº”
                        message_content.append({
                            "type": "markdown",
                            "data": chat_response
                        })
                        
                        # æ„å»ºæ ‡å‡†åŒ–å“åº”æ ¼å¼
                        standardized_response = {
                            "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
                            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                            "role": "assistant",
                            "content": message_content,
                            "legacy_data": response_data
                        }
                        
                        return jsonify({
                            'success': True,
                            'data': standardized_response
                        })
                        
                    except Exception as chat_error:
                        logger.error(f"æ™®é€šèŠå¤©å¤„ç†å¤±è´¥: {chat_error}")
                        return jsonify({
                            'success': False,
                            'error': f"èŠå¤©å¤„ç†å¤±è´¥: {str(chat_error)}"
                        }), 500
                
                elif intent_type == 'mcp_action' and confidence > confidence_threshold:
                    # MCPè°ƒç”¨ï¼šä½¿ç”¨æ ‡å‡†MCPç³»ç»Ÿ
                    logger.info(f"æ‰§è¡Œæ ‡å‡†MCPæ“ä½œ: {query_text}")
                    skip_search = True
                    
                    # æ­¥éª¤1ï¼šæ£€æŸ¥MCPå¼€å…³çŠ¶æ€
                    mcp_enabled = request.json.get('enable_mcp', False)
                    if not mcp_enabled:
                        logger.warning("MCPåŠŸèƒ½å·²å…³é—­")
                        
                        message_content = [{
                            "type": "text",
                            "data": "âš ï¸ MCPåŠŸèƒ½å·²å…³é—­ï¼Œæ— æ³•æ‰§è¡ŒMCPå·¥å…·æ“ä½œ\nğŸ’¡ è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨MCPåŠŸèƒ½åé‡è¯•"
                        }]
                        
                        standardized_response = {
                            "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
                            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                            "role": "assistant",
                            "content": message_content
                        }
                        
                        return jsonify({'success': True, 'data': standardized_response})
                    
                    # æ­¥éª¤2ï¼šä½¿ç”¨æ ‡å‡†MCPç³»ç»Ÿåˆ†æå’Œæ‰§è¡Œå·¥å…·
                    try:
                        from app.services.mcp.servers.mcp_manager import MCPManager
                        from app.services.mcp_tool_analyzer import create_mcp_tool_analyzer
                        
                        # åˆ›å»ºMCPç®¡ç†å™¨å®ä¾‹
                        mcp_manager = MCPManager()
                        
                        # åˆå§‹åŒ–MCPç³»ç»Ÿï¼ˆå¦‚æœæœªåˆå§‹åŒ–ï¼‰
                        if not mcp_manager.is_initialized:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            init_success = loop.run_until_complete(mcp_manager.initialize())
                            loop.close()
                            
                            if not init_success:
                                message_content = [{
                                    "type": "text",
                                    "data": "âŒ MCPç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œå·¥å…·æ“ä½œ"
                                }]
                                
                                standardized_response = {
                                    "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
                                    "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                                    "role": "assistant",
                                    "content": message_content
                                }
                                
                                return jsonify({'success': True, 'data': standardized_response})
                        
                        # åˆ›å»ºå¸¦æœ‰MCPç®¡ç†å™¨çš„å·¥å…·åˆ†æå™¨
                        mcp_tool_analyzer = create_mcp_tool_analyzer(mcp_manager)
                        
                        # ä½¿ç”¨LLMåˆ†æéœ€è¦çš„å·¥å…·
                        tool_analysis = mcp_tool_analyzer.analyze_tools_needed(query_text)
                        logger.info(f"å·¥å…·åˆ†æç»“æœ: {tool_analysis}")
                        
                        # è·å–åˆ†æçš„å·¥å…·åˆ—è¡¨
                        tools_needed = tool_analysis.get('tools_needed', [])
                        if not tools_needed:
                            message_content = [{
                                "type": "text",
                                "data": "âŒ æœªèƒ½è¯†åˆ«å‡ºéœ€è¦æ‰§è¡Œçš„å…·ä½“å·¥å…·"
                            }]
                        else:
                            # æ‰§è¡Œå·¥å…·åºåˆ—
                            message_content = []
                            
                            # æ·»åŠ å·¥å…·åˆ†æç»“æœ
                            message_content.append({
                                "type": "text",
                                "data": f"ğŸ¯ è¯†åˆ«ä¸ºMCPæ“ä½œ: {intent_analysis.get('reasoning', 'å·²è¯†åˆ«ä¸ºMCPæ“ä½œ')}"
                            })
                            
                            if tool_analysis.get('reasoning'):
                                message_content.append({
                                    "type": "text",
                                    "data": f"ğŸ”§ å·¥å…·åˆ†æ: {tool_analysis['reasoning']}"
                                })
                            
                            # æŒ‰æ‰§è¡Œåºåˆ—æ‰§è¡Œå·¥å…·ï¼Œç¡®ä¿æ‰€æœ‰å·¥å…·éƒ½èƒ½æ‰§è¡Œ
                            execution_sequence = tool_analysis.get('execution_sequence', [])
                            if execution_sequence:
                                # æŒ‰åºåˆ—æ‰§è¡Œ
                                for step in execution_sequence:
                                    tool_name = step.get('tool_name')
                                    tool_arguments = step.get('parameters', {})
                                    
                                    if not tool_name:
                                        continue
                                        
                                    try:
                                        logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_arguments}")
                                        
                                        # è°ƒç”¨æ ‡å‡†MCPå·¥å…·
                                        loop = asyncio.new_event_loop()
                                        asyncio.set_event_loop(loop)
                                        result = loop.run_until_complete(
                                            mcp_manager.call_tool(tool_name, tool_arguments)
                                        )
                                        loop.close()
                                        
                                        # å¤„ç†æ‰§è¡Œç»“æœ
                                        if result.isError:
                                            error_msg = result.content[0].get('text', 'Unknown error') if result.content else 'Unknown error'
                                            logger.error(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {error_msg}")
                                            message_content.append({
                                                "type": "text",
                                                "data": f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {error_msg}"
                                            })
                                        else:
                                            # æˆåŠŸæ‰§è¡Œ
                                            tool_output = result.content[0].get('text', 'æ“ä½œå®Œæˆ') if result.content else 'æ“ä½œå®Œæˆ'
                                            logger.info(f"å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ: {tool_output}")
                                            message_content.append({
                                                "type": "tool_call",
                                                "data": {
                                                    "tool": tool_name,
                                                    "params": tool_arguments,
                                                    "result": tool_output,
                                                    "user_visible": True
                                                }
                                            })
                                            
                                    except Exception as tool_error:
                                        logger.error(f"æ‰§è¡Œå·¥å…· {tool_name} å¼‚å¸¸: {tool_error}")
                                        message_content.append({
                                            "type": "text",
                                            "data": f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¼‚å¸¸: {str(tool_error)}"
                                        })
                            else:
                                # é™çº§åˆ°æŒ‰å·¥å…·åˆ—è¡¨æ‰§è¡Œ
                                for tool_name in tools_needed:
                                    try:
                                        logger.info(f"é™çº§æ‰§è¡Œå·¥å…·: {tool_name}")
                                        
                                        # è°ƒç”¨æ ‡å‡†MCPå·¥å…·
                                        loop = asyncio.new_event_loop()
                                        asyncio.set_event_loop(loop)
                                        result = loop.run_until_complete(
                                            mcp_manager.call_tool(tool_name, {})
                                        )
                                        loop.close()
                                        
                                        # å¤„ç†æ‰§è¡Œç»“æœ
                                        if result.isError:
                                            error_msg = result.content[0].get('text', 'Unknown error') if result.content else 'Unknown error'
                                            message_content.append({
                                                "type": "text",
                                                "data": f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {error_msg}"
                                            })
                                        else:
                                            # æˆåŠŸæ‰§è¡Œ
                                            tool_output = result.content[0].get('text', 'æ“ä½œå®Œæˆ') if result.content else 'æ“ä½œå®Œæˆ'
                                            message_content.append({
                                                "type": "tool_call",
                                                "data": {
                                                    "tool": tool_name,
                                                    "params": {},
                                                    "result": tool_output,
                                                    "user_visible": True
                                                }
                                            })
                                            
                                    except Exception as tool_error:
                                        logger.error(f"æ‰§è¡Œå·¥å…· {tool_name} å¤±è´¥: {tool_error}")
                                        message_content.append({
                                            "type": "text",
                                            "data": f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¼‚å¸¸: {str(tool_error)}"
                                        })
                        
                        # æ”¶é›†æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœ
                        mcp_results = []
                        for i, content_item in enumerate(message_content):
                            if content_item.get("type") == "tool_call":
                                mcp_results.append({
                                    'tool_name': content_item["data"]["tool"],
                                    'arguments': content_item["data"]["params"],
                                    'result': content_item["data"]["result"],
                                    'error': None,
                                    'timestamp': time.time()
                                })
                            elif content_item.get("type") == "text" and "âŒ" in content_item.get("data", ""):
                                # å¤„ç†å¤±è´¥çš„å·¥å…·è°ƒç”¨
                                tool_name = tools_needed[0] if tools_needed else "unknown"
                                mcp_results.append({
                                    'tool_name': tool_name,
                                    'arguments': {},
                                    'result': None,
                                    'error': content_item["data"],
                                    'timestamp': time.time()
                                })
                        
                        # æ„å»ºæ ‡å‡†åŒ–å“åº”æ ¼å¼
                        standardized_response = {
                            "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
                            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                            "role": "assistant",
                            "content": message_content,
                            "legacy_data": {
                                'query': query_text,
                                'search_type': 'mcp_action',
                                'intent_analysis': intent_analysis,
                                'tool_analysis': tool_analysis,
                                'mcp_system': 'standard',
                                'mcp_results': mcp_results
                            }
                        }
                        
                        return jsonify({
                            'success': True,
                            'data': standardized_response
                        })
                        
                    except Exception as mcp_error:
                        logger.error(f"æ ‡å‡†MCPç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {mcp_error}")
                        
                        # é”™è¯¯æƒ…å†µä¹Ÿè¿”å›æ ‡å‡†åŒ–æ ¼å¼
                        message_content = [{
                            "type": "text",
                            "data": f"âŒ MCPæ“ä½œå¤±è´¥: {str(mcp_error)}"
                        }]
                        
                        standardized_response = {
                            "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
                            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                            "role": "assistant",
                            "content": message_content,
                            "legacy_data": {'error': str(mcp_error)}
                        }
                        
                        return jsonify({
                            'success': True,
                            'data': standardized_response
                        })
                        
                # å¯¹äº knowledge_search æ„å›¾ï¼Œç»§ç»­æ‰§è¡Œå‘é‡æ£€ç´¢ï¼ˆä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                elif intent_type == 'knowledge_search':
                    logger.info(f"æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢æ“ä½œ: {query_text}")
                    # ç»§ç»­å‘ä¸‹æ‰§è¡Œè¯­ä¹‰æœç´¢é€»è¾‘
                    pass
                else:
                    # ç½®ä¿¡åº¦ä¸å¤Ÿæˆ–æœªè¯†åˆ«çš„æ„å›¾ï¼Œé»˜è®¤æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢
                    logger.info(f"æ„å›¾ä¸æ˜ç¡®æˆ–ç½®ä¿¡åº¦ä¸è¶³ï¼Œæ‰§è¡Œé»˜è®¤çŸ¥è¯†åº“æ£€ç´¢: {query_text}")
                        
            except Exception as e:
                logger.error(f"æ„å›¾åˆ†æå¤±è´¥: {e}")
                # é™çº§åˆ°çŸ¥è¯†åº“æ£€ç´¢
                intent_analysis = {
                    "intent_type": "knowledge_search",
                    "confidence": 0.5,
                    "reasoning": f"æ„å›¾åˆ†æå¤±è´¥ï¼Œé™çº§åˆ°çŸ¥è¯†åº“æ£€ç´¢: {str(e)}",
                    "used_llm": False,
                    "error": str(e),
                    "model_used": f"{Config.INTENT_ANALYSIS_LLM_PROVIDER}:{Config.INTENT_ANALYSIS_LLM_MODEL}",
                    "prompt_source": "config_file"
                }
        
        # åªæœ‰åœ¨æ²¡æœ‰ç‰¹æ®Šå¤„ç†æ—¶æ‰æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ï¼‰
        if not skip_search:
            try:
                # è¿›è¡Œå…³é”®è¯æå–ä»¥ä¼˜åŒ–æœç´¢
                keyword_extraction = None
                search_query = query_text  # é»˜è®¤ä½¿ç”¨åŸæŸ¥è¯¢
                
                if llm_model:
                    try:
                        keyword_extraction = LLMService.extract_search_keywords(query_text, llm_model)
                        search_query = keyword_extraction.get('optimized_query', query_text)
                        logger.info(f"å…³é”®è¯æå–å®Œæˆ - åŸæŸ¥è¯¢: {query_text}, ä¼˜åŒ–æŸ¥è¯¢: {search_query}")
                    except Exception as e:
                        logger.warning(f"å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸæŸ¥è¯¢: {e}")
                        search_query = query_text
                
                # è®¾ç½®åˆç†çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé¿å…è¿”å›å¤ªå¤šä½è´¨é‡ç»“æœï¼‰
                min_score = 0.2  # è¯­ä¹‰æœç´¢çš„é»˜è®¤æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                
                # å¯¹äººåç­‰ä¸“æœ‰åè¯æŸ¥è¯¢é€‚å½“é™ä½é˜ˆå€¼
                if len(search_query.strip().split()) <= 2 and not any(word in search_query.lower() for word in ['åˆ›å»º', 'æ–°å»º', 'å¸®æˆ‘', 'æ‰¾åˆ°', 'æœç´¢']):
                    min_score = 0.15  # ç®€çŸ­æŸ¥è¯¢ï¼ˆå¦‚äººåï¼‰é™ä½é˜ˆå€¼
                    logger.info(f"æ£€æµ‹åˆ°ç®€çŸ­æŸ¥è¯¢ï¼Œé™ä½ç›¸ä¼¼åº¦é˜ˆå€¼åˆ°: {min_score}")
                
                # ä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢è¿›è¡Œå‘é‡æœç´¢
                vector_service = VectorServiceAdapter()
                raw_results = vector_service.search_similar(
                    query_text=search_query, 
                    top_k=top_k,
                    min_score=min_score
                )
                
                # è¡¥å……æ–‡æ¡£ä¿¡æ¯ï¼Œç¡®ä¿å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤ºç»“æœ
                search_results = []
                for result in raw_results:
                    document_id = result.get('document_id')
                    if document_id:
                        document = DocumentNode.query.get(document_id)
                        if document and not document.is_deleted:
                            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ å®Œæ•´çš„documentå¯¹è±¡
                            enriched_result = {
                                'id': result.get('chunk_id', ''),
                                'document_id': document_id,
                                'chunk_id': result.get('chunk_id', ''),
                                'text': result.get('text', ''),
                                'score': result.get('score', 0),
                                'search_type': 'semantic',  # æ·»åŠ æœç´¢ç±»å‹æ ‡è¯†
                                'document': {
                                    'id': document.id,
                                    'name': document.name,
                                    'file_type': document.file_type,
                                    'file_size': document.file_size,
                                    'created_at': document.created_at.isoformat() if document.created_at else None,
                                    'description': document.description
                                }
                            }
                            search_results.append(enriched_result)
                
                logger.info(f"è¯­ä¹‰æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                
                # å¦‚æœæœ‰LLMæ¨¡å‹ï¼Œè¿›è¡Œç»“æœèšåˆå’Œæ™ºèƒ½åˆ†æ
                file_results = []
                llm_answer = None
                reranked = False
                
                if llm_model and search_results:
                    try:
                        # å°†chunkçº§åˆ«ç»“æœèšåˆä¸ºæ–‡ä»¶çº§åˆ«ï¼ˆå¤ç”¨æ··åˆæœç´¢çš„èšåˆé€»è¾‘ï¼‰
                        file_results = aggregate_results_by_file(search_results)
                        
                        # LLMç»“æœé‡æ’åº
                        if file_results:
                            try:
                                file_results = LLMService.rerank_file_results(query_text, file_results, llm_model)
                                reranked = True
                                logger.info(f"è¯­ä¹‰æœç´¢LLMæ–‡ä»¶ç»“æœé‡æ’åºå®Œæˆ - å¤„ç†äº† {len(file_results)} ä¸ªæ–‡ä»¶")
                            except Exception as e:
                                logger.warning(f"è¯­ä¹‰æœç´¢LLMç»“æœé‡æ’åºå¤±è´¥: {e}")
                        
                        # LLMæ™ºèƒ½ç­”æ¡ˆç”Ÿæˆ
                        if file_results:
                            try:
                                # å‡†å¤‡ä¸Šä¸‹æ–‡æ–‡æœ¬
                                context_texts = []
                                for file_result in file_results[:5]:  # åªä½¿ç”¨å‰5ä¸ªæ–‡ä»¶ç»“æœç”Ÿæˆç­”æ¡ˆ
                                    doc_name = file_result['document']['name']
                                    # åˆå¹¶è¯¥æ–‡ä»¶çš„æ‰€æœ‰åŒ¹é…ç‰‡æ®µ
                                    chunks = [chunk['text'] for chunk in file_result['chunks'][:3]]  # æ¯ä¸ªæ–‡ä»¶æœ€å¤š3ä¸ªç‰‡æ®µ
                                    combined_text = '\n'.join(chunks)
                                    context_texts.append(f"æ–‡æ¡£ã€Š{doc_name}ã€‹ï¼š{combined_text}")
                                
                                context = '\n\n'.join(context_texts)
                                llm_answer = LLMService.generate_answer(
                                    query_text, 
                                    context, 
                                    llm_model=llm_model,
                                    scenario=None,  # è®©ç³»ç»Ÿè‡ªåŠ¨åˆ†æ
                                    style=None      # è®©ç³»ç»Ÿè‡ªåŠ¨æ¨è
                                )
                                logger.info(f"è¯­ä¹‰æœç´¢æ™ºèƒ½LLMç­”æ¡ˆç”Ÿæˆå®Œæˆ - æŸ¥è¯¢: {query_text}")
                            except Exception as e:
                                logger.warning(f"è¯­ä¹‰æœç´¢LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
                    except Exception as e:
                        logger.warning(f"è¯­ä¹‰æœç´¢LLMå¤„ç†å¤±è´¥: {e}")
                
            except Exception as e:
                logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {str(e)}")
                search_results = []
                keyword_extraction = None
                file_results = []
                llm_answer = None
                reranked = False
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'query': query_text,
            'results': file_results,
            'file_results': file_results,  # ä¿æŒå‘åå…¼å®¹
            'total_results': len(file_results),
            'search_type': 'semantic',
            'similarity_level': 'medium',  # é»˜è®¤ç›¸ä¼¼åº¦çº§åˆ«
            'min_score': min_score if not skip_search else 0.0,
            'optimized_query': search_query if not skip_search else query_text,
            'original_query': query_text,
            'reranked': reranked,
            'mcp_results': mcp_tool_results if mcp_tool_results else [],
            'llm_info': {
                'model': llm_model,
                'answer': llm_answer if 'llm_answer' in locals() else None,
                'enabled': llm_model is not None
            } if 'llm_answer' in locals() and llm_answer else None
        }
        
        # æ·»åŠ æ„å›¾åˆ†æç»“æœ
        if intent_analysis:
            response_data['intent_analysis'] = {
                'intent_type': intent_analysis.get('intent_type'),
                'confidence': intent_analysis.get('confidence'),
                'action_type': intent_analysis.get('action_type'),
                'reasoning': intent_analysis.get('reasoning'),
                'used_llm': True,
                'model': intent_analysis.get('model_used'),
                'prompt_source': intent_analysis.get('prompt_source', 'config_file')
            }
        
        # æ·»åŠ å…³é”®è¯æå–ä¿¡æ¯
        if 'keyword_extraction' in locals() and keyword_extraction:
            response_data['keyword_extraction'] = keyword_extraction
        
        # è½¬æ¢ä¸ºæ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼
        message_content = []
        
        # 1. æ·»åŠ æ„å›¾åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if intent_analysis and intent_analysis.get('confidence', 0) > Config.INTENT_ANALYSIS_CONFIDENCE_THRESHOLD:
            message_content.append({
                "type": "text",
                "data": f"ğŸ¯ æ„å›¾è¯†åˆ«: {intent_analysis.get('reasoning', 'å·²è¯†åˆ«ç”¨æˆ·æ„å›¾')}"
            })
        
        # 2. æ·»åŠ LLMç­”æ¡ˆï¼ˆå¦‚æœæœ‰ï¼‰
        if 'llm_answer' in locals() and llm_answer:
            message_content.append({
                "type": "markdown",
                "data": llm_answer
            })
        
        # 3. æ·»åŠ MCPå·¥å…·æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if mcp_tool_results:
            for mcp_result in mcp_tool_results:
                # å¤„ç†dataclasså¯¹è±¡æˆ–å­—å…¸
                error = getattr(mcp_result, 'error', None) if hasattr(mcp_result, 'error') else mcp_result.get('error') if isinstance(mcp_result, dict) else None
                if error:
                    message_content.append({
                        "type": "text",
                        "data": f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {error}"
                    })
                else:
                    tool_name = getattr(mcp_result, 'tool_name', None) if hasattr(mcp_result, 'tool_name') else mcp_result.get('tool_name', 'unknown') if isinstance(mcp_result, dict) else 'unknown'
                    arguments = getattr(mcp_result, 'arguments', {}) if hasattr(mcp_result, 'arguments') else mcp_result.get('arguments', {}) if isinstance(mcp_result, dict) else {}
                    result = getattr(mcp_result, 'result', None) if hasattr(mcp_result, 'result') else mcp_result.get('result') if isinstance(mcp_result, dict) else None
                    
                    message_content.append({
                        "type": "tool_call",
                        "data": {
                            "tool": tool_name,
                            "params": arguments,
                            "result": result,
                            "user_visible": True
                        }
                    })
        
        # 4. æ·»åŠ æœç´¢ç»“æœè¡¨æ ¼ï¼ˆå¦‚æœæœ‰æ–‡ä»¶ç»“æœï¼‰
        if file_results:
            # æ„å»ºæœç´¢ç»“æœè¡¨æ ¼
            table_headers = ["æ–‡æ¡£", "ç›¸å…³åº¦", "æœç´¢ç±»å‹", "å†…å®¹é¢„è§ˆ"]
            table_rows = []
            
            for file_result in file_results[:5]:  # é™åˆ¶æ˜¾ç¤ºå‰5ä¸ªç»“æœ
                document = file_result.get('document', {})
                
                # è®¡ç®—å¹³å‡ç›¸å…³åº¦
                score = file_result.get('score', 0)
                score_display = f"{score:.2f}" if score > 0 else "N/A"
                
                # æœç´¢ç±»å‹
                search_types = file_result.get('search_types', ['unknown'])
                search_type_display = "+".join(search_types)
                
                # å†…å®¹é¢„è§ˆï¼ˆä½¿ç”¨æœ€ä½³chunkï¼‰
                chunks = file_result.get('chunks', [])
                content_preview = chunks[0].get('text', '')[:100] + "..." if chunks else "æ— é¢„è§ˆ"
                
                # åˆ›å»ºå¯ç‚¹å‡»çš„æ–‡ä»¶åï¼ˆå¸¦æ–‡æ¡£IDç”¨äºå‰ç«¯ç‚¹å‡»å¤„ç†ï¼‰
                file_name_with_link = {
                    "text": document.get('name', 'æœªçŸ¥æ–‡æ¡£'),
                    "document_id": document.get('id')
                }
                
                table_rows.append([
                    file_name_with_link,
                    score_display,
                    search_type_display,
                    content_preview
                ])
            
            if table_rows:
                message_content.append({
                    "type": "table",
                    "data": {
                        "headers": table_headers,
                        "rows": table_rows
                    }
                })
        
        # 6. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œæ·»åŠ å»ºè®®
        if not file_results and not mcp_tool_results:
            message_content.append({
                "type": "text",
                "data": "ğŸ˜” æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå»ºè®®ï¼š"
            })
            message_content.append({
                "type": "markdown",
                "data": f"""
## ğŸ’¡ æœç´¢å»ºè®®

- **è°ƒæ•´å…³é”®è¯**: å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„è¯æ±‡
- **é™ä½ç›¸ä¼¼åº¦**: å½“å‰è®¾ç½®ä¸º `medium`ï¼Œå¯å°è¯• `low` æˆ– `any`
- **æ£€æŸ¥æ‹¼å†™**: ç¡®è®¤æœç´¢è¯æ‹¼å†™æ­£ç¡®
- **ä½¿ç”¨åŒä¹‰è¯**: å°è¯•ç›¸å…³çš„åŒä¹‰è¯æˆ–è¿‘ä¹‰è¯

**åŸå§‹æŸ¥è¯¢**: `{query_text}`  
**ä¼˜åŒ–æŸ¥è¯¢**: `{search_query if not skip_search else query_text}`
"""
            })
        
        # æ„å»ºæ ‡å‡†åŒ–å“åº”æ ¼å¼
        standardized_response = {
            "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "role": "assistant",
            "content": message_content,
            # ä¿æŒåŸæœ‰æ•°æ®ç»“æ„ä»¥ç¡®ä¿å‘åå…¼å®¹
            "legacy_data": response_data
        }
        
        return jsonify({
            'success': True,
            'data': standardized_response
        })
        
    except Exception as e:
        logger.error(f"è¯­ä¹‰æœç´¢è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'æœç´¢å¤±è´¥: {str(e)}'
        }), 500

@search_bp.route('/documents', methods=['GET'])
def search_documents():
    """æ–‡æ¡£åç§°æœç´¢"""
    try:
        query = request.args.get('q', '').strip()
        file_type = request.args.get('type')
        parent_id = request.args.get('parent_id', type=int)
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æŸ¥è¯¢å‚æ•°ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # æ„å»ºæŸ¥è¯¢
        db_query = DocumentNode.query.filter_by(is_deleted=False)
        
        # æŒ‰åç§°æ¨¡ç³Šæœç´¢
        db_query = db_query.filter(DocumentNode.name.contains(query))
        
        # æŒ‰æ–‡ä»¶ç±»å‹è¿‡æ»¤
        if file_type:
            db_query = db_query.filter_by(file_type=file_type)
        
        # æŒ‰çˆ¶ç›®å½•è¿‡æ»¤
        if parent_id is not None:
            db_query = db_query.filter_by(parent_id=parent_id)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        documents = db_query.order_by(DocumentNode.type.desc(), DocumentNode.name).limit(50).all()
        
        results = [doc.to_dict() for doc in documents]
        
        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'total_results': len(results),
                'results': results
            }
        })
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@search_bp.route('/suggest', methods=['GET'])
def search_suggestions():
    """æœç´¢å»ºè®®"""
    try:
        query = request.args.get('q', '').strip()
        limit = request.args.get('limit', 10, type=int)
        
        if not query or len(query) < 2:
            return jsonify({
                'success': True,
                'data': []
            })
        
        # æŸ¥è¯¢åŒ¹é…çš„æ–‡æ¡£åç§°
        documents = DocumentNode.query.filter(
            DocumentNode.name.contains(query),
            DocumentNode.is_deleted == False
        ).limit(limit).all()
        
        suggestions = []
        for doc in documents:
            suggestions.append({
                'id': doc.id,
                'name': doc.name,
                'type': doc.type,
                'file_type': doc.file_type
            })
        
        return jsonify({
            'success': True,
            'data': suggestions
        })
        
    except Exception as e:
        logger.error(f"æœç´¢å»ºè®®å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@search_bp.route('/stats', methods=['GET'])
def search_stats():
    """æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        vector_service = VectorServiceAdapter()
        
        # è·å–å‘é‡ç»Ÿè®¡
        vector_stats = vector_service.get_collection_stats()
        
        # è·å–æ–‡æ¡£ç»Ÿè®¡
        total_documents = DocumentNode.query.filter_by(is_deleted=False, type='file').count()
        total_folders = DocumentNode.query.filter_by(is_deleted=False, type='folder').count()
        
        # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        file_type_stats = {}
        for doc in DocumentNode.query.filter_by(is_deleted=False, type='file').all():
            file_type = doc.file_type or 'unknown'
            file_type_stats[file_type] = file_type_stats.get(file_type, 0) + 1
        
        return jsonify({
            'success': True,
            'data': {
                'vector_stats': vector_stats,
                'document_stats': {
                    'total_documents': total_documents,
                    'total_folders': total_folders,
                    'file_type_distribution': file_type_stats
                }
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–æœç´¢ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@search_bp.route('/hybrid', methods=['POST'])
def hybrid_search():
    """æ··åˆæœç´¢ï¼šè¯­ä¹‰æœç´¢ + å…³é”®è¯æœç´¢"""
    try:
        data = request.get_json()
        query_text = data.get('query')
        top_k = data.get('top_k', 10)
        document_id = data.get('document_id')
        similarity_level = data.get('similarity_level', 'medium')
        
        # LLMç›¸å…³å‚æ•°
        use_llm = data.get('enable_llm', data.get('use_llm', False))  # å…¼å®¹ä¸¤ç§å‚æ•°å
        llm_model = data.get('llm_model')
        enable_intent_analysis = data.get('enable_intent_analysis', True)
        
        logger.info(f"æ”¶åˆ°æ··åˆæœç´¢è¯·æ±‚ - query: {query_text}, similarity_level: {similarity_level}, use_llm: {use_llm}, llm_model: {llm_model}")
        
        if not query_text:
            return jsonify({
                'success': False,
                'error': 'æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # åˆå§‹åŒ–ç»“æœå˜é‡
        semantic_results = []
        keyword_results = []
        combined_results = []
        file_results = []
        mcp_tool_results = []
        skip_search = False
        intent_analysis = None
        
        # ä½¿ç”¨ä¸“ç”¨çš„æ„å›¾è¯†åˆ«æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ„å›¾åˆ†æ
        if Config.ENABLE_INTENT_ANALYSIS and enable_intent_analysis:
            try:
                from app.services.llm import LLMService
                # ä½¿ç”¨ä¸“ç”¨çš„æ„å›¾è¯†åˆ«æ¨¡å‹
                intent_llm_model = f"{Config.INTENT_ANALYSIS_LLM_PROVIDER}:{Config.INTENT_ANALYSIS_LLM_MODEL}"
                intent_analysis = LLMService.analyze_user_intent(query_text, intent_llm_model)
                logger.info(f"æ„å›¾åˆ†æç»“æœ: {intent_analysis}")
                
                # ä½¿ç”¨é…ç½®çš„ç½®ä¿¡åº¦é˜ˆå€¼
                confidence_threshold = Config.INTENT_ANALYSIS_CONFIDENCE_THRESHOLD
                
                # æ ¹æ®æ„å›¾åˆ†æç»“æœå†³å®šå¤„ç†æ–¹å¼
                if (intent_analysis.get('intent_type') == 'folder_analysis' and 
                    intent_analysis.get('confidence', 0) > confidence_threshold and
                    intent_analysis.get('action_type') == 'analyze_folder'):
                    
                    logger.info(f"æ‰§è¡Œæ–‡ä»¶å¤¹åˆ†ææ“ä½œ: {query_text}")
                    skip_search = True
                    
                    # æ‰§è¡Œæ–‡ä»¶å¤¹åˆ†æ
                    from app.services.folder_analysis_service import FolderAnalysisService
                    
                    try:
                        # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„LLMæ¨¡å‹è¿›è¡Œåˆ†æï¼Œè€Œä¸æ˜¯æ„å›¾è¯†åˆ«æ¨¡å‹
                        analysis_result = FolderAnalysisService.analyze_folder_completeness(
                            query_text, llm_model or intent_llm_model, intent_analysis
                        )
                        logger.info(f"æ–‡ä»¶å¤¹åˆ†æå®Œæˆ")
                        
                        # æ„å»ºåˆ†æå“åº”
                        response_data = {
                            'query': query_text,
                            'is_analysis': True,
                            'analysis_result': analysis_result,
                            'search_type': 'folder_analysis',
                            'intent_analysis': {
                                'intent_type': intent_analysis.get('intent_type'),
                                'confidence': intent_analysis.get('confidence'),
                                'action_type': intent_analysis.get('action_type'),
                                'reasoning': intent_analysis.get('reasoning'),
                                'used_llm': True,
                                'model': intent_analysis.get('model_used', intent_llm_model),
                                'prompt_source': intent_analysis.get('prompt_source', 'config_file'),
                                'confidence_threshold': confidence_threshold
                            }
                        }
                        
                        return jsonify({
                            'success': True,
                            'data': response_data
                        })
                        
                    except Exception as analysis_error:
                        logger.error(f"æ–‡ä»¶å¤¹åˆ†æå¤±è´¥: {analysis_error}")
                        return jsonify({
                            'success': False,
                            'error': f"æ–‡ä»¶å¤¹åˆ†æå¤±è´¥: {str(analysis_error)}"
                        }), 500
                
                elif (intent_analysis.get('intent_type') == 'mcp_action' and 
                    intent_analysis.get('confidence', 0) > confidence_threshold and
                    intent_analysis.get('action_type') in ['create_file', 'create_folder']):
                    
                    logger.info(f"æ‰§è¡ŒMCPæ“ä½œ: {query_text}")
                    skip_search = True
                    
                    # ä½¿ç”¨å®Œæ•´çš„MCPå·¥å…·åˆ†æå’Œæ‰§è¡Œæµç¨‹
                    from app.services.mcp_tool_analyzer import create_mcp_tool_analyzer
                    from app.services.mcp_tool_executor import mcp_tool_executor
                    
                    # åˆ›å»ºå·¥å…·åˆ†æå™¨å®ä¾‹
                    mcp_tool_analyzer = create_mcp_tool_analyzer()
                    
                    # 1. åˆ†æéœ€è¦çš„å·¥å…·
                    tool_analysis = mcp_tool_analyzer.analyze_tools_needed(query_text)
                    
                    # 2. éªŒè¯å·¥å…·å¯ç”¨æ€§
                    tool_validation = mcp_tool_analyzer.validate_tools(tool_analysis.get('tools_needed', []))
                    
                    if not tool_validation['all_valid']:
                        # æœ‰ä¸å¯ç”¨çš„å·¥å…·ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                        error_msg = mcp_tool_analyzer.get_error_message(tool_validation['error_type'], tool_validation.get('invalid_tools', []))
                        mcp_tool_results = [{'tool_name': 'error', 'arguments': {}, 'result': None, 'error': error_msg, 'timestamp': time.time()}]
                        logger.warning(f"MCPå·¥å…·éªŒè¯å¤±è´¥: {error_msg}")
                    else:
                        # 3. æ‰§è¡Œå·¥å…·
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            mcp_tool_results = loop.run_until_complete(
                                mcp_tool_executor.execute_tools_from_analysis(query_text, tool_analysis)
                            )
                            logger.info(f"åŸºäºLLMåˆ†æçš„MCPå·¥å…·æ‰§è¡Œå®Œæˆï¼Œå…± {len(mcp_tool_results)} ä¸ªæ­¥éª¤")
                        except Exception as mcp_error:
                            logger.error(f"MCPå·¥å…·æ‰§è¡Œå¤±è´¥: {mcp_error}")
                            mcp_tool_results = [{
                                'tool_name': 'error',
                                'arguments': {},
                                'result': None,
                                'error': f"MCPå·¥å…·æ‰§è¡Œå¤±è´¥: {str(mcp_error)}",
                                'timestamp': time.time()
                            }]
                        finally:
                            loop.close()
                        
            except Exception as e:
                logger.error(f"æ„å›¾åˆ†æå¤±è´¥: {e}")
                # é™çº§åˆ°å‘é‡æœç´¢
                intent_analysis = {
                    "intent_type": "vector_search",
                    "confidence": 0.5,
                    "action_type": "search_documents",
                    "reasoning": f"æ„å›¾åˆ†æå¤±è´¥ï¼Œé™çº§åˆ°å‘é‡æœç´¢: {str(e)}",
                    "used_llm": False,
                    "error": str(e),
                    "model_used": f"{Config.INTENT_ANALYSIS_LLM_PROVIDER}:{Config.INTENT_ANALYSIS_LLM_MODEL}",
                    "prompt_source": "config_file"
                }
        
        # åªæœ‰åœ¨æ²¡æœ‰MCPæ“ä½œæ—¶æ‰æ‰§è¡Œæœç´¢
        if not skip_search:
            # è¿›è¡Œå…³é”®è¯æå–ä»¥ä¼˜åŒ–æœç´¢
            original_query = query_text
            optimized_query = query_text
            keyword_extraction = None
            
            if use_llm and llm_model:
                try:
                    # ä½¿ç”¨ä¸“é—¨çš„å…³é”®è¯æå–åŠŸèƒ½
                    keyword_extraction = LLMService.extract_search_keywords(query_text, llm_model)
                    optimized_query = keyword_extraction.get('optimized_query', query_text)
                    logger.info(f"æ··åˆæœç´¢å…³é”®è¯æå–å®Œæˆ - åŸæŸ¥è¯¢: {query_text}, ä¼˜åŒ–æŸ¥è¯¢: {optimized_query}")
                except Exception as e:
                    logger.warning(f"æ··åˆæœç´¢å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸæŸ¥è¯¢: {e}")
                    optimized_query = query_text
            
            # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé’ˆå¯¹é“¶è¡Œç­‰ç‰¹å®šæŸ¥è¯¢æé«˜é˜ˆå€¼ï¼‰
            similarity_thresholds = {
                'high': 0.6, 'medium': 0.3, 'low': 0.1, 'any': 0.0
            }
            min_score = similarity_thresholds.get(similarity_level, 0.3)
            
            # å¯¹ç‰¹å®šæŸ¥è¯¢ç±»å‹åŠ¨æ€è°ƒæ•´é˜ˆå€¼
            if any(word in query_text.lower() for word in ['é“¶è¡Œ', 'é‡‘èæœºæ„', 'è´·æ¬¾']):
                min_score = max(min_score, 0.4)  # é“¶è¡Œç›¸å…³æŸ¥è¯¢æé«˜æœ€ä½é˜ˆå€¼
                logger.info(f"æ£€æµ‹åˆ°é“¶è¡Œç›¸å…³æŸ¥è¯¢ï¼Œæé«˜é˜ˆå€¼åˆ°: {min_score}")
            
            # åˆå§‹åŒ–å‘é‡æœåŠ¡
            milvus_host = SystemConfig.get_config('milvus_host', 'localhost')
            milvus_port = SystemConfig.get_config('milvus_port', 19530)
            embedding_model = SystemConfig.get_config('embedding_model')
            
            vector_service = VectorServiceAdapter(
                milvus_host=milvus_host,
                milvus_port=milvus_port,
                embedding_model=embedding_model
            )
            
            # 1. æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼‰
            semantic_results = vector_service.search_similar(
                query_text=optimized_query,
                top_k=top_k,
                document_id=document_id,
                min_score=min_score
            )
            
            # 2. æ‰§è¡Œå…³é”®è¯æœç´¢ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼‰
            keyword_results = vector_service.search_by_keywords(
                query_text=optimized_query,
                top_k=top_k,
                document_id=document_id
            )
            
            # 3. åˆå¹¶å’Œå»é‡ç»“æœ
            combined_results = merge_search_results(semantic_results, keyword_results, query_text)
            
            # 4. ä¸°å¯Œç»“æœä¿¡æ¯å¹¶æŒ‰æ–‡ä»¶èšåˆ
            file_results = aggregate_results_by_file(combined_results)
        else:
            # å¦‚æœè·³è¿‡æœç´¢ï¼Œåˆå§‹åŒ–å˜é‡ä»¥é¿å…åç»­å¤„ç†ä¸­çš„é”™è¯¯
            original_query = query_text
            optimized_query = query_text
            min_score = 0.0
            file_results = []  # åˆå§‹åŒ–ç©ºçš„æ–‡ä»¶ç»“æœåˆ—è¡¨
            semantic_results = []  # åˆå§‹åŒ–ç©ºçš„è¯­ä¹‰æœç´¢ç»“æœ
            keyword_results = []   # åˆå§‹åŒ–ç©ºçš„å…³é”®è¯æœç´¢ç»“æœ
        
        # 5. LLMç»“æœé‡æ’åºï¼ˆåŸºäºæ–‡ä»¶ï¼‰
        reranked = False
        if use_llm and llm_model and file_results:
            try:
                file_results = LLMService.rerank_file_results(original_query, file_results, llm_model)
                reranked = True
                logger.info(f"æ··åˆæœç´¢LLMæ–‡ä»¶ç»“æœé‡æ’åºå®Œæˆ - å¤„ç†äº† {len(file_results)} ä¸ªæ–‡ä»¶")
            except Exception as e:
                logger.warning(f"æ··åˆæœç´¢LLMç»“æœé‡æ’åºå¤±è´¥: {e}")
        
        # 6. LLMç­”æ¡ˆç”Ÿæˆï¼ˆä½¿ç”¨æ™ºèƒ½æç¤ºè¯ç³»ç»Ÿï¼‰
        llm_answer = None
        if use_llm and llm_model and file_results:
            try:
                # å‡†å¤‡ä¸Šä¸‹æ–‡æ–‡æœ¬
                context_texts = []
                for file_result in file_results[:5]:  # åªä½¿ç”¨å‰5ä¸ªæ–‡ä»¶ç»“æœç”Ÿæˆç­”æ¡ˆ
                    doc_name = file_result['document']['name']
                    # åˆå¹¶è¯¥æ–‡ä»¶çš„æ‰€æœ‰åŒ¹é…ç‰‡æ®µ
                    chunks = [chunk['text'] for chunk in file_result['chunks'][:3]]  # æ¯ä¸ªæ–‡ä»¶æœ€å¤š3ä¸ªç‰‡æ®µ
                    combined_text = '\n'.join(chunks)
                    context_texts.append(f"æ–‡æ¡£ã€Š{doc_name}ã€‹ï¼š{combined_text}")
                
                context = '\n\n'.join(context_texts)
                llm_answer = LLMService.generate_answer(
                    original_query, 
                    context, 
                    llm_model=llm_model,
                    scenario=None,  # è®©ç³»ç»Ÿè‡ªåŠ¨åˆ†æ
                    style=None      # è®©ç³»ç»Ÿè‡ªåŠ¨æ¨è
                )
                logger.info(f"æ··åˆæœç´¢æ™ºèƒ½LLMç­”æ¡ˆç”Ÿæˆå®Œæˆ - æŸ¥è¯¢: {original_query}")
            except Exception as e:
                logger.warning(f"æ··åˆæœç´¢LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")

        # 7. MCPå·¥å…·è°ƒç”¨å·²åœ¨å‰é¢å¤„ç†
        
        response_data = {
            'query': query_text,
            'similarity_level': similarity_level,
            'total_files': len(file_results),
            'file_results': file_results,
            'search_type': 'hybrid'
        }
        
        # åªåœ¨æ‰§è¡Œäº†æœç´¢æ—¶æ‰æ·»åŠ è¿™äº›ä¿¡æ¯
        if not skip_search:
            response_data.update({
                'min_score': min_score,
                'semantic_count': len(semantic_results),
                'keyword_count': len(keyword_results)
            })
        
        # æ·»åŠ æ„å›¾åˆ†æç»“æœ
        if intent_analysis:
            response_data['intent_analysis'] = {
                'intent_type': intent_analysis.get('intent_type'),
                'confidence': intent_analysis.get('confidence'),
                'action_type': intent_analysis.get('action_type'),
                'reasoning': intent_analysis.get('reasoning'),
                'used_llm': True
            }
        
        # æ·»åŠ å…³é”®è¯æå–ç»“æœ
        if not skip_search and 'keyword_extraction' in locals() and keyword_extraction:
            response_data['keyword_extraction'] = {
                'original_query': keyword_extraction.get('original_query'),
                'keywords': keyword_extraction.get('keywords'),
                'optimized_query': keyword_extraction.get('optimized_query'),
                'reasoning': keyword_extraction.get('reasoning'),
                'used_llm': keyword_extraction.get('used_llm', False)
            }
        
        # æ·»åŠ LLMå¤„ç†ä¿¡æ¯
        if use_llm:
            response_data['llm_info'] = {
                'used': True,
                'model': llm_model,
                'original_query': original_query if not skip_search else query_text,
                'optimized_query': optimized_query if not skip_search else query_text,
                'query_optimized': (optimized_query != original_query) if not skip_search else False,
                'reranked': reranked,
                'answer': llm_answer
            }
        else:
            response_data['llm_info'] = {
                'used': False
            }
        
        # æ·»åŠ MCPå·¥å…·ç»“æœ
        if mcp_tool_results:
            response_data['mcp_results'] = [{
                'tool_name': result.tool_name if hasattr(result, 'tool_name') else result.get('tool_name'),
                'arguments': result.arguments if hasattr(result, 'arguments') else result.get('arguments', {}),
                'result': result.result if hasattr(result, 'result') else result.get('result'),
                'error': result.error if hasattr(result, 'error') else result.get('error'),
                'timestamp': result.timestamp if hasattr(result, 'timestamp') else result.get('timestamp')
            } for result in mcp_tool_results]
        
        logger.info(f"æ··åˆæœç´¢å®Œæˆ - semantic: {len(semantic_results)}, keyword: {len(keyword_results)}, files: {len(file_results)}")
        
        # è½¬æ¢ä¸ºæ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼
        message_content = []
        
        # 1. æ·»åŠ æ„å›¾åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if intent_analysis and intent_analysis.get('confidence', 0) > Config.INTENT_ANALYSIS_CONFIDENCE_THRESHOLD:
            message_content.append({
                "type": "text",
                "data": f"ğŸ¯ æ„å›¾è¯†åˆ«: {intent_analysis.get('reasoning', 'å·²è¯†åˆ«ç”¨æˆ·æ„å›¾')}"
            })
        
        # 2. æ·»åŠ LLMç­”æ¡ˆï¼ˆå¦‚æœæœ‰ï¼‰
        if 'llm_answer' in locals() and llm_answer:
            message_content.append({
                "type": "markdown",
                "data": llm_answer
            })
        
        # 3. æ·»åŠ MCPå·¥å…·æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if mcp_tool_results:
            for mcp_result in mcp_tool_results:
                # å¤„ç†dataclasså¯¹è±¡æˆ–å­—å…¸
                error = getattr(mcp_result, 'error', None) if hasattr(mcp_result, 'error') else mcp_result.get('error') if isinstance(mcp_result, dict) else None
                if error:
                    message_content.append({
                        "type": "text",
                        "data": f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {error}"
                    })
                else:
                    tool_name = getattr(mcp_result, 'tool_name', None) if hasattr(mcp_result, 'tool_name') else mcp_result.get('tool_name', 'unknown') if isinstance(mcp_result, dict) else 'unknown'
                    arguments = getattr(mcp_result, 'arguments', {}) if hasattr(mcp_result, 'arguments') else mcp_result.get('arguments', {}) if isinstance(mcp_result, dict) else {}
                    result = getattr(mcp_result, 'result', None) if hasattr(mcp_result, 'result') else mcp_result.get('result') if isinstance(mcp_result, dict) else None
                    
                    message_content.append({
                        "type": "tool_call",
                        "data": {
                            "tool": tool_name,
                            "params": arguments,
                            "result": result,
                            "user_visible": True
                        }
                    })
        
        # 4. æ·»åŠ æœç´¢ç»“æœè¡¨æ ¼ï¼ˆå¦‚æœæœ‰æ–‡ä»¶ç»“æœï¼‰
        if file_results:
            # æ„å»ºæœç´¢ç»“æœè¡¨æ ¼
            table_headers = ["æ–‡æ¡£", "ç›¸å…³åº¦", "æœç´¢ç±»å‹", "å†…å®¹é¢„è§ˆ"]
            table_rows = []
            
            for file_result in file_results[:5]:  # é™åˆ¶æ˜¾ç¤ºå‰5ä¸ªç»“æœ
                document = file_result.get('document', {})
                
                # è®¡ç®—å¹³å‡ç›¸å…³åº¦
                score = file_result.get('score', 0)
                score_display = f"{score:.2f}" if score > 0 else "N/A"
                
                # æœç´¢ç±»å‹
                search_types = file_result.get('search_types', ['unknown'])
                search_type_display = "+".join(search_types)
                
                # å†…å®¹é¢„è§ˆï¼ˆä½¿ç”¨æœ€ä½³chunkï¼‰
                chunks = file_result.get('chunks', [])
                content_preview = chunks[0].get('text', '')[:100] + "..." if chunks else "æ— é¢„è§ˆ"
                
                # åˆ›å»ºå¯ç‚¹å‡»çš„æ–‡ä»¶åï¼ˆå¸¦æ–‡æ¡£IDç”¨äºå‰ç«¯ç‚¹å‡»å¤„ç†ï¼‰
                file_name_with_link = {
                    "text": document.get('name', 'æœªçŸ¥æ–‡æ¡£'),
                    "document_id": document.get('id')
                }
                
                table_rows.append([
                    file_name_with_link,
                    score_display,
                    search_type_display,
                    content_preview
                ])
            
            if table_rows:
                message_content.append({
                    "type": "table",
                    "data": {
                        "headers": table_headers,
                        "rows": table_rows
                    }
                })
        
        # 6. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œæ·»åŠ å»ºè®®
        if not file_results and not mcp_tool_results:
            message_content.append({
                "type": "text",
                "data": "ğŸ˜” æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå»ºè®®ï¼š"
            })
            message_content.append({
                "type": "markdown",
                "data": f"""
## ğŸ’¡ æœç´¢å»ºè®®

- **è°ƒæ•´å…³é”®è¯**: å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„è¯æ±‡
- **é™ä½ç›¸ä¼¼åº¦**: å½“å‰è®¾ç½®ä¸º `{similarity_level}`ï¼Œå¯å°è¯• `low` æˆ– `any`
- **æ£€æŸ¥æ‹¼å†™**: ç¡®è®¤æœç´¢è¯æ‹¼å†™æ­£ç¡®
- **ä½¿ç”¨åŒä¹‰è¯**: å°è¯•ç›¸å…³çš„åŒä¹‰è¯æˆ–è¿‘ä¹‰è¯

**åŸå§‹æŸ¥è¯¢**: `{original_query}`  
**ä¼˜åŒ–æŸ¥è¯¢**: `{optimized_query}`
"""
            })
        
        # æ„å»ºæ ‡å‡†åŒ–å“åº”æ ¼å¼
        standardized_response = {
            "message_id": f"msg-{int(time.time())}-{hash(query_text) % 1000:03d}",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "role": "assistant",
            "content": message_content,
            # ä¿æŒåŸæœ‰æ•°æ®ç»“æ„ä»¥ç¡®ä¿å‘åå…¼å®¹
            "legacy_data": response_data
        }
        
        return jsonify({
            'success': True,
            'data': standardized_response
        })
        
    except Exception as e:
        logger.error(f"æ··åˆæœç´¢å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def merge_search_results(semantic_results, keyword_results, query_text):
    """åˆå¹¶è¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢ç»“æœ"""
    try:
        # ä½¿ç”¨æ–‡æ¡£ID+chunk_idä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
        seen_results = {}
        merged_results = []
        
        # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        for result in semantic_results:
            key = f"{result['document_id']}_{result.get('chunk_id', '')}"
            result['search_type'] = 'semantic'
            seen_results[key] = result
            merged_results.append(result)
        
        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
        for result in keyword_results:
            key = f"{result['document_id']}_{result.get('chunk_id', '')}"
            if key not in seen_results:
                result['search_type'] = 'keyword'
                merged_results.append(result)
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°æœç´¢ç±»å‹ä¸ºæ··åˆ
                seen_results[key]['search_type'] = 'hybrid'
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        merged_results.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        return merged_results
        
    except Exception as e:
        logger.error(f"åˆå¹¶æœç´¢ç»“æœå¤±è´¥: {str(e)}")
        return semantic_results  # fallbackåˆ°è¯­ä¹‰æœç´¢ç»“æœ

def aggregate_results_by_file(chunk_results):
    """å°†chunkçº§åˆ«çš„æœç´¢ç»“æœèšåˆä¸ºæ–‡ä»¶çº§åˆ«"""
    try:
        file_groups = {}
        
        # æŒ‰æ–‡æ¡£IDåˆ†ç»„
        for result in chunk_results:
            doc_id = str(result['document_id'])
            if doc_id not in file_groups:
                file_groups[doc_id] = []
            file_groups[doc_id].append(result)
        
        file_results = []
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºèšåˆç»“æœ
        for doc_id, chunks in file_groups.items():
            document = DocumentNode.query.get(doc_id)
            if not document or document.is_deleted:
                continue
            
            # è®¡ç®—æ–‡ä»¶çº§åˆ«çš„ç»¼åˆå¾—åˆ†
            scores = [chunk.get('score', 0) for chunk in chunks]
            max_score = max(scores) if scores else 0
            avg_score = sum(scores) / len(scores) if scores else 0
            combined_score = max_score * 0.7 + avg_score * 0.3  # æƒé‡ç»„åˆ
            
            # æŒ‰å¾—åˆ†å¯¹chunksæ’åº
            chunks.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # åˆ›å»ºæ–‡ä»¶çº§åˆ«çš„ç»“æœ
            file_result = {
                'document': {
                    'id': document.id,
                    'name': document.name,
                    'file_type': document.file_type,
                    'file_path': document.file_path,
                    'description': document.description,
                    'file_size': document.file_size,
                    'parent_id': document.parent_id
                },
                'score': combined_score,
                'max_chunk_score': max_score,
                'chunk_count': len(chunks),
                'chunks': [
                    {
                        'chunk_id': chunk.get('chunk_id'),
                        'text': chunk.get('text', ''),
                        'score': chunk.get('score', 0),
                        'search_type': chunk.get('search_type', 'unknown')
                    }
                    for chunk in chunks
                ],
                'search_types': list(set(chunk.get('search_type', 'unknown') for chunk in chunks))
            }
            
            file_results.append(file_result)
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        file_results.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"æ–‡ä»¶èšåˆå®Œæˆ - ä» {len(chunk_results)} ä¸ªchunkèšåˆä¸º {len(file_results)} ä¸ªæ–‡ä»¶")
        return file_results
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶èšåˆå¤±è´¥: {str(e)}")
        return []

def detect_analysis_intent(query_text):
    """æ£€æµ‹æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«æ–‡æ¡£åˆ†ææ„å›¾"""
    import re
    
    # åˆ†ææ„å›¾å…³é”®è¯ - æ‰©å±•æ›´å¤šæ¨¡å¼
    analysis_keywords = [
        # åŸºæœ¬åˆ†ææ¨¡å¼
        r'åˆ†æ.*?æ–‡æ¡£', r'åˆ†æ.*?æ–‡ä»¶', r'åˆ†æ.*?èµ„æ–™',
        r'æ–‡æ¡£.*?åˆ†æ', r'æ–‡ä»¶.*?åˆ†æ', r'èµ„æ–™.*?åˆ†æ',
        r'åˆ†æ.*?ç›®å½•', r'ç›®å½•.*?åˆ†æ',
        r'åˆ†æ.*?æ–‡ä»¶å¤¹', r'æ–‡ä»¶å¤¹.*?åˆ†æ',
        
        # æ£€æŸ¥å’Œè¯„ä¼°ç±»
        r'æ£€æŸ¥.*?æ–‡æ¡£', r'æ£€æŸ¥.*?æ–‡ä»¶', r'æ£€æŸ¥.*?ç»“æ„', r'æ£€æŸ¥.*?æ–‡ä»¶å¤¹',
        r'è¯„ä¼°.*?æ–‡æ¡£', r'è¯„ä¼°.*?æ–‡ä»¶', r'è¯„ä¼°.*?ç»“æ„', r'è¯„ä¼°.*?æ–‡ä»¶å¤¹',
        
        # ç¼ºå¤±åˆ†æç±» - æ–°å¢
        r'.*?ç¼ºå°‘.*?å†…å®¹', r'.*?ç¼ºå¤±.*?å†…å®¹', r'.*?è¿˜éœ€è¦.*?',
        r'.*?è¿˜ç¼º.*?', r'.*?å°‘.*?å†…å®¹', r'.*?ä¸è¶³.*?',
        
        # å®Œæ•´æ€§æ£€æŸ¥ç±» - æ–°å¢  
        r'.*?å®Œæ•´.*?æ£€æŸ¥', r'.*?å®Œæ•´.*?åˆ†æ', r'.*?é½å…¨.*?',
        r'.*?å†…å®¹.*?å®Œæ•´', r'.*?ææ–™.*?é½å…¨',
        
        # ç›´æ¥åˆ†ææ¨¡å¼ - æ–°å¢
        r'^åˆ†æ\s*[^\s]{1,30}$',  # åˆ†æXXXï¼ˆç®€çŸ­æ–‡æ¡£åï¼‰
        r'^åˆ†æ\s*.{1,50}$'       # åˆ†æä»»æ„å†…å®¹ï¼ˆä¸­ç­‰é•¿åº¦ï¼‰
    ]
    
    query_lower = query_text.lower().strip()
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ†ææ„å›¾
    for pattern in analysis_keywords:
        if re.search(pattern, query_lower):
            # å°è¯•æå–æ–‡æ¡£åç§°
            doc_name = extract_document_name(query_text, pattern)
            return {
                'is_analysis': True,
                'document_name': doc_name,
                'original_query': query_text,
                'matched_pattern': pattern
            }
    
    return {'is_analysis': False}

def extract_document_name(query_text, matched_pattern):
    """ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–æ–‡æ¡£åç§°"""
    import re
    
    # å¸¸è§çš„æ–‡æ¡£åç§°æå–æ¨¡å¼ - æ‰©å±•æ›´å¤šæ¨¡å¼
    patterns = [
        # å¼•ç”¨æ ¼å¼
        r'åˆ†æ["\']([^"\']+)["\']',  # åˆ†æ"æ–‡æ¡£å"
        r'åˆ†æã€Š([^ã€‹]+)ã€‹',         # åˆ†æã€Šæ–‡æ¡£åã€‹
        
        # åŸºæœ¬åˆ†ææ ¼å¼
        r'åˆ†æ(?:æ–‡æ¡£|æ–‡ä»¶|èµ„æ–™|æ–‡ä»¶å¤¹)?[ï¼š:]?\s*([^\sï¼Œã€‚ï¼ï¼Ÿ]+)',  # åˆ†ææ–‡æ¡£ï¼šåç§° æˆ– åˆ†æ åç§°
        r'([^\sï¼Œã€‚ï¼ï¼Ÿ]+)(?:æ–‡æ¡£|æ–‡ä»¶|èµ„æ–™|æ–‡ä»¶å¤¹)?.*?åˆ†æ',  # åç§°æ–‡æ¡£åˆ†æ
        r'åˆ†æ.*?([^\sï¼Œã€‚ï¼ï¼Ÿ]{2,20})(?:æ–‡æ¡£|æ–‡ä»¶|èµ„æ–™|ç›®å½•|æ–‡ä»¶å¤¹)',  # åˆ†æXXæ–‡æ¡£
        
        # ç¼ºå¤±åˆ†æç±» - æ–°å¢
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+).*?(?:ç¼ºå°‘|ç¼ºå¤±|è¿˜éœ€è¦|è¿˜ç¼º|å°‘|ä¸è¶³).*?å†…å®¹',  # XXç¼ºå°‘å†…å®¹
        r'.*?ç¼ºå°‘.*?([^ï¼Œã€‚ï¼ï¼Ÿ\s]+).*?å†…å®¹',  # ç¼ºå°‘XXå†…å®¹
        
        # å®Œæ•´æ€§æ£€æŸ¥ç±»
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+).*?(?:å®Œæ•´|é½å…¨)',  # XXå®Œæ•´
        r'(?:å®Œæ•´|é½å…¨).*?([^ï¼Œã€‚ï¼ï¼Ÿ\s]+)',  # å®Œæ•´XX
        
        # é€šç”¨æ¨¡å¼ - åŒ¹é…æ–‡æ¡£IDæˆ–åç§°
        r'åˆ†æ\s*([0-9]+[^\sï¼Œã€‚ï¼ï¼Ÿ]*)',  # åˆ†æ01æ¯•ç£Š
        r'åˆ†æ\s*([^\sï¼Œã€‚ï¼ï¼Ÿ]{2,30})',    # åˆ†æä»»æ„åç§°
        
        # ä»æ–‡ä»¶å¤¹æè¿°ä¸­æå–
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+)\s*(?:è¿™ä¸ª|é‚£ä¸ª|è¯¥|æ­¤)?\s*(?:æ–‡ä»¶å¤¹|ç›®å½•)',  # XXæ–‡ä»¶å¤¹
    ]
    
    # æŒ‰ä¼˜å…ˆçº§å°è¯•æå–
    for pattern in patterns:
        match = re.search(pattern, query_text)
        if match:
            doc_name = match.group(1).strip()
            
            # æ¸…ç†å¸¸è§çš„åœç”¨è¯å’Œæ— æ„ä¹‰è¯æ±‡
            doc_name = re.sub(r'^(è¿™ä¸ª|é‚£ä¸ª|è¯¥|æ­¤|ä¸€ä¸‹|ä¸‹|çš„)\s*', '', doc_name)
            doc_name = re.sub(r'\s*(çš„|å§|å‘¢|å•Š|å—|æ–‡æ¡£|æ–‡ä»¶|èµ„æ–™|æ–‡ä»¶å¤¹|ç›®å½•)$', '', doc_name)
            
            # ç¡®ä¿æå–çš„åç§°æœ‰æ„ä¹‰ï¼ˆé•¿åº¦>=1ï¼Œä¸å…¨æ˜¯æ ‡ç‚¹ç¬¦å·ï¼‰
            if len(doc_name) >= 1 and re.search(r'[a-zA-Z0-9\u4e00-\u9fff]', doc_name):
                return doc_name
    
    # å¦‚æœä»¥ä¸Šéƒ½æ²¡åŒ¹é…ï¼Œå°è¯•ç®€å•çš„ç©ºæ ¼åˆ†å‰²
    words = query_text.split()
    if len(words) >= 2 and words[0] in ['åˆ†æ', 'æ£€æŸ¥', 'è¯„ä¼°']:
        # ä»ç¬¬äºŒä¸ªè¯å¼€å§‹ç»„åˆå¯èƒ½çš„æ–‡æ¡£å
        for i in range(1, len(words)):
            candidate = ''.join(words[1:i+1])
            if re.search(r'[a-zA-Z0-9\u4e00-\u9fff]', candidate) and len(candidate) >= 1:
                return candidate
    
    return None

def handle_document_analysis(analysis_intent, llm_model):
    """å¤„ç†æ–‡æ¡£åˆ†æè¯·æ±‚"""
    try:
        doc_name = analysis_intent.get('document_name')
        original_query = analysis_intent.get('original_query')
        
        if not doc_name:
            return jsonify({
                'success': True,
                'data': {
                    'query': original_query,
                    'is_analysis': True,
                    'analysis_result': {
                        'type': 'error',
                        'message': 'æœªèƒ½ä»æŸ¥è¯¢ä¸­è¯†åˆ«å‡ºå…·ä½“çš„æ–‡æ¡£åç§°ã€‚è¯·æ˜ç¡®æŒ‡å®šè¦åˆ†æçš„æ–‡æ¡£ï¼Œä¾‹å¦‚ï¼š"åˆ†æé¡¹ç›®æ–‡æ¡£"æˆ–"åˆ†æã€ŠåˆåŒç®¡ç†ã€‹æ–‡ä»¶"ã€‚'
                    }
                }
            })
        
        # æœç´¢åŒ¹é…çš„æ–‡æ¡£
        documents = DocumentNode.query.filter(
            DocumentNode.name.contains(doc_name),
            DocumentNode.is_deleted == False
        ).limit(10).all()
        
        if not documents:
            return jsonify({
                'success': True,
                'data': {
                    'query': original_query,
                    'is_analysis': True,
                    'analysis_result': {
                        'type': 'not_found',
                        'message': f'æœªæ‰¾åˆ°åç§°åŒ…å«"{doc_name}"çš„æ–‡æ¡£ã€‚è¯·æ£€æŸ¥æ–‡æ¡£åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…ä½¿ç”¨æ–‡æ¡£æœç´¢åŠŸèƒ½æŸ¥çœ‹å¯ç”¨çš„æ–‡æ¡£åˆ—è¡¨ã€‚',
                        'searched_name': doc_name
                    }
                }
            })
        
        # å¦‚æœæ‰¾åˆ°å¤šä¸ªæ–‡æ¡£ï¼Œè®©ç”¨æˆ·é€‰æ‹©
        if len(documents) > 1:
            doc_list = []
            for doc in documents:
                doc_data = doc.to_dict()
                # æ·»åŠ è·¯å¾„ä¿¡æ¯
                path_parts = []
                current = doc
                while current.parent_id:
                    parent = DocumentNode.query.get(current.parent_id)
                    if parent and not parent.is_deleted:
                        path_parts.append(parent.name)
                        current = parent
                    else:
                        break
                doc_data['path'] = ' > '.join(reversed(path_parts)) if path_parts else 'æ ¹ç›®å½•'
                doc_list.append(doc_data)
            
            return jsonify({
                'success': True,
                'data': {
                    'query': original_query,
                    'is_analysis': True,
                    'analysis_result': {
                        'type': 'multiple_found',
                        'message': f'æ‰¾åˆ°{len(documents)}ä¸ªåŒ…å«"{doc_name}"çš„æ–‡æ¡£ï¼Œè¯·æ˜ç¡®æŒ‡å®šè¦åˆ†æçš„æ–‡æ¡£ï¼š',
                        'documents': doc_list,
                        'searched_name': doc_name
                    }
                }
            })
        
        # æ‰¾åˆ°å”¯ä¸€æ–‡æ¡£ï¼Œæ‰§è¡Œåˆ†æ
        document = documents[0]
        
        if not llm_model:
            return jsonify({
                'success': True,
                'data': {
                    'query': original_query,
                    'is_analysis': True,
                    'analysis_result': {
                        'type': 'no_model',
                        'message': 'æ–‡æ¡£åˆ†æéœ€è¦é€‰æ‹©AIæ¨¡å‹ã€‚è¯·åœ¨è®¾ç½®ä¸­é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„AIæ¨¡å‹åé‡è¯•ã€‚',
                        'document': document.to_dict()
                    }
                }
            })
        
        # æ‰§è¡Œæ–‡æ¡£åˆ†æ
        try:
            # è·å–æ–‡æ¡£æ ‡ç­¾ä¿¡æ¯
            document_tags = [tag.to_dict() for tag in document.tags]
            
            # æ„å»ºæ–‡æ¡£æ ‘ç»“æ„
            def build_document_tree(node):
                result = {
                    'id': node.id,
                    'name': node.name,
                    'type': node.type,
                    'file_type': node.file_type,
                    'description': node.description,
                    'children': []
                }
                
                children = DocumentNode.query.filter_by(
                    parent_id=node.id,
                    is_deleted=False
                ).order_by(DocumentNode.type.desc(), DocumentNode.name).all()
                
                for child in children:
                    result['children'].append(build_document_tree(child))
                
                return result
            
            document_tree = build_document_tree(document)
            
            # è°ƒç”¨LLMåˆ†ææœåŠ¡
            analysis_result = LLMService.analyze_document_structure(
                document_name=document.name,
                document_tags=document_tags,
                document_tree=document_tree,
                llm_model=llm_model
            )
            
            if not analysis_result:
                return jsonify({
                    'success': True,
                    'data': {
                        'query': original_query,
                        'is_analysis': True,
                        'analysis_result': {
                            'type': 'analysis_failed',
                            'message': 'AIæ¨¡å‹åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–å°è¯•å…¶ä»–AIæ¨¡å‹ã€‚',
                            'document': document.to_dict()
                        }
                    }
                })
            
            # è¿”å›åˆ†æç»“æœ
            return jsonify({
                'success': True,
                'data': {
                    'query': original_query,
                    'is_analysis': True,
                    'analysis_result': {
                        'type': 'success',
                        'message': f'å·²å®Œæˆå¯¹æ–‡æ¡£ã€Š{document.name}ã€‹çš„æ™ºèƒ½åˆ†æï¼š',
                        'document': document.to_dict(),
                        'analysis': analysis_result,
                        'llm_model': llm_model
                    }
                }
            })
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
            return jsonify({
                'success': True,
                'data': {
                    'query': original_query,
                    'is_analysis': True,
                    'analysis_result': {
                        'type': 'error',
                        'message': f'æ–‡æ¡£åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}',
                        'document': document.to_dict()
                    }
                }
            })
            
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æ¡£åˆ†æè¯·æ±‚å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'å¤„ç†æ–‡æ¡£åˆ†æè¯·æ±‚å¤±è´¥: {str(e)}'
        }), 500


def _extract_tool_arguments(tool_info: dict, query_text: str) -> dict:
    """ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–å·¥å…·å‚æ•°"""
    tool_name = tool_info.get('name', '')
    parameters = tool_info.get('parameters', {})
    arguments = {}
    
    try:
        # æ ¹æ®å·¥å…·ç±»å‹å’Œå‚æ•°å®šä¹‰æå–å‚æ•°
        if tool_name == 'navigate':
            # æå–URL
            import re
            url_pattern = r'https?://[^\s]+'
            url_match = re.search(url_pattern, query_text)
            if url_match:
                arguments['url'] = url_match.group()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´URLï¼Œå¯èƒ½éœ€è¦ç”¨æˆ·è¡¥å……
                arguments['url'] = 'https://www.example.com'
        
        elif tool_name == 'web_search':
            # å¯¹äºç½‘ç»œæœç´¢ï¼Œç›´æ¥ä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬
            arguments['query'] = query_text
            arguments['num_results'] = 5
        
        elif tool_name == 'screenshot':
            # æˆªå›¾å·¥å…·é€šå¸¸ä¸éœ€è¦ç‰¹å®šå‚æ•°
            arguments['full_page'] = True
        
        elif tool_name == 'click':
            # ç‚¹å‡»éœ€è¦é€‰æ‹©å™¨ï¼Œä»æŸ¥è¯¢ä¸­æå–
            if 'æŒ‰é’®' in query_text:
                arguments['selector'] = 'button'
            elif 'é“¾æ¥' in query_text:
                arguments['selector'] = 'a'
            else:
                arguments['selector'] = '*'
        
        elif tool_name == 'fill_form':
            # è¡¨å•å¡«å†™éœ€è¦é€‰æ‹©å™¨å’Œå€¼
            arguments['selector'] = 'input'
            arguments['value'] = 'ç¤ºä¾‹å€¼'
        
        elif tool_name == 'get_page_content':
            # è·å–é¡µé¢å†…å®¹
            import re
            url_pattern = r'https?://[^\s]+'
            url_match = re.search(url_pattern, query_text)
            if url_match:
                arguments['url'] = url_match.group()
            else:
                arguments['url'] = 'https://www.example.com'
        
        # å¦‚æœå·¥å…·æœ‰å¿…éœ€å‚æ•°ä½†æœªæå–åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
        for param_name, param_info in parameters.items():
            if param_name not in arguments and not param_info.get('optional', False):
                if param_info.get('type') == 'string':
                    arguments[param_name] = f"ä»æŸ¥è¯¢æå–: {query_text[:50]}"
                elif param_info.get('type') == 'integer':
                    arguments[param_name] = param_info.get('default', 1)
                elif param_info.get('type') == 'boolean':
                    arguments[param_name] = param_info.get('default', True)
        
        logger.info(f"æå–å·¥å…·å‚æ•° - å·¥å…·: {tool_name}, å‚æ•°: {arguments}")
        return arguments
        
    except Exception as e:
        logger.warning(f"æå–å·¥å…·å‚æ•°å¤±è´¥: {e}")
        return {} 