"""
åŸºç¡€å‘é‡åŒ–å™¨
å®šä¹‰æ‰€æœ‰å‘é‡åŒ–å™¨çš„é€šç”¨æ¥å£å’ŒåŠŸèƒ½
"""

import logging
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple
import os
import uuid
from datetime import datetime
import threading

logger = logging.getLogger(__name__)

# å…¨å±€æ¨¡å‹å®ä¾‹å’Œé”ï¼Œé¿å…é‡å¤åŠ è½½
_model_instance = None
_model_lock = threading.Lock()

class BaseVectorizer(ABC):
    """åŸºç¡€å‘é‡åŒ–å™¨æŠ½è±¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŸºç¡€å‘é‡åŒ–å™¨"""
        self.collection_name = "document_vectors"
        self.dimension = None  # æ”¹ä¸ºåŠ¨æ€æ£€æµ‹ï¼Œä¸å†ç¡¬ç¼–ç 
        self.model = None
        self.is_available = False
        self.enabled = os.getenv('ENABLE_VECTOR_SERVICE', 'false').lower() == 'true'
        
        # ä»é…ç½®æ–‡ä»¶è·å–Milvusè¿æ¥ä¿¡æ¯
        self.milvus_host = os.getenv('MILVUS_HOST', 'localhost')
        self.milvus_port = int(os.getenv('MILVUS_PORT', '19530'))
        
        if not self.enabled:
            logger.warning("Vector service is disabled by configuration")
    
    def _detect_model_dimension(self, model) -> int:
        """æ£€æµ‹æ¨¡å‹çš„å®é™…ç»´åº¦"""
        try:
            if model is None:
                logger.warning("Model is None, using default dimension 384")
                return 384
            
            # ä½¿ç”¨æµ‹è¯•æ–‡æœ¬è·å–å‘é‡ç»´åº¦
            test_text = "This is a test sentence for dimension detection."
            test_vector = model.encode(test_text, normalize_embeddings=True)
            
            if hasattr(test_vector, 'shape'):
                dimension = test_vector.shape[0] if len(test_vector.shape) == 1 else test_vector.shape[-1]
            else:
                dimension = len(test_vector) if isinstance(test_vector, (list, tuple)) else len(test_vector.tolist())
            
            logger.info(f"ğŸ” æ£€æµ‹åˆ°åµŒå…¥æ¨¡å‹ç»´åº¦: {dimension}")
            return dimension
            
        except Exception as e:
            logger.error(f"Failed to detect model dimension: {e}")
            logger.warning("Using default dimension 384")
            return 384
    
    def _get_existing_collection_dimension(self) -> Optional[int]:
        """è·å–ç°æœ‰é›†åˆçš„å‘é‡ç»´åº¦"""
        try:
            from pymilvus import Collection, utility
            
            if not utility.has_collection(self.collection_name):
                return None
            
            collection = Collection(self.collection_name)
            schema = collection.schema
            
            for field in schema.fields:
                if field.name == "vector":
                    dimension = field.params.get('dim')
                    logger.info(f"ğŸ“Š ç°æœ‰é›†åˆ '{self.collection_name}' å‘é‡ç»´åº¦: {dimension}")
                    return dimension
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get existing collection dimension: {e}")
            return None
    
    def _handle_dimension_mismatch(self, model_dim: int, collection_dim: int) -> bool:
        """å¤„ç†ç»´åº¦ä¸åŒ¹é…é—®é¢˜"""
        try:
            from pymilvus import utility
            
            logger.warning(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…: æ¨¡å‹ç»´åº¦={model_dim}, é›†åˆç»´åº¦={collection_dim}")
            logger.info(f"ğŸ”„ åˆ é™¤ç°æœ‰é›†åˆå¹¶é‡æ–°åˆ›å»º...")
            
            # åˆ é™¤ç°æœ‰é›†åˆ
            if utility.has_collection(self.collection_name):
                utility.drop_collection(self.collection_name)
                logger.info(f"âœ… å·²åˆ é™¤é›†åˆ '{self.collection_name}'")
            
            # æ›´æ–°ç»´åº¦ä¸ºæ¨¡å‹å®é™…ç»´åº¦
            self.dimension = model_dim
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            return self._create_collection_with_dimension(model_dim)
            
        except Exception as e:
            logger.error(f"Failed to handle dimension mismatch: {e}")
            return False
    
    def _create_collection_with_dimension(self, dimension: int) -> bool:
        """ä½¿ç”¨æŒ‡å®šç»´åº¦åˆ›å»ºé›†åˆ"""
        try:
            from pymilvus import Collection, FieldSchema, CollectionSchema, DataType
            
            logger.info(f"ğŸš€ åˆ›å»ºæ–°é›†åˆï¼Œç»´åº¦: {dimension}")
            
            # å®šä¹‰å­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ]
            
            # åˆ›å»ºé›†åˆæ¶æ„
            schema = CollectionSchema(fields, f"Document vectors collection with {dimension}D embeddings")
            
            # åˆ›å»ºé›†åˆ
            collection = Collection(self.collection_name, schema)
            
            # åˆ›å»ºç´¢å¼•
            index_params = {
                "metric_type": "COSINE",
                "index_type": "IVF_FLAT",
                "params": {"nlist": 128}
            }
            
            collection.create_index("vector", index_params)
            logger.info(f"âœ… åˆ›å»ºé›†åˆ '{self.collection_name}' æˆåŠŸï¼Œç»´åº¦: {dimension}")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to create collection with dimension {dimension}: {e}")
            return False
    
    def _load_embedding_model(self, model_name: str = 'all-MiniLM-L6-v2'):
        """å®‰å…¨åœ°åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
        global _model_instance, _model_lock
        
        with _model_lock:
            # å¦‚æœå·²æœ‰å®ä¾‹ï¼Œç›´æ¥ä½¿ç”¨
            if _model_instance is not None:
                logger.info("Using existing embedding model instance")
                return _model_instance
            
            try:
                logger.info(f"Loading text embedding model: {model_name}")
                from sentence_transformers import SentenceTransformer
                
                # è®¾ç½®ç¯å¢ƒå˜é‡é¿å…å¹¶å‘é—®é¢˜å’Œè­¦å‘Š
                os.environ['TOKENIZERS_PARALLELISM'] = 'false'
                
                # è®¾ç½®torchç¯å¢ƒå˜é‡ä»¥é¿å…pin_memoryè­¦å‘Š
                import torch
                if not torch.cuda.is_available():
                    # å¦‚æœæ²¡æœ‰GPUï¼Œå¼ºåˆ¶ç¦ç”¨pin_memory
                    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
                
                # åŠ è½½æ¨¡å‹
                _model_instance = SentenceTransformer(model_name)
                logger.info("âœ… Embedding model loaded successfully")
                return _model_instance
                
            except ImportError:
                logger.warning("sentence-transformers not available, will use mock vectors")
                return None
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                # æ¸…é™¤å¯èƒ½æŸåçš„å®ä¾‹
                _model_instance = None
                return None
    
    def initialize_vector_service(self, host: str = None, port: int = None, model_name: str = None):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥å’Œæ¨¡å‹"""
        if not self.enabled:
            logger.info("Vector service initialization skipped - disabled by configuration")
            return True
            
        try:
            # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            host = host or self.milvus_host
            port = port or self.milvus_port
            model_name = model_name or os.getenv('EMBEDDING_MODEL', 'all-MiniLM-L6-v2')
            
            logger.info(f"Initializing vector service - connecting to {host}:{port}")
            logger.info(f"ğŸ¤– ä½¿ç”¨åµŒå…¥æ¨¡å‹: {model_name}")
            
            from pymilvus import connections, utility
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¿æ¥ï¼Œå¦‚æœå­˜åœ¨ä¸”é…ç½®ä¸åŒåˆ™å…ˆæ–­å¼€
            if connections.has_connection("default"):
                try:
                    # å°è¯•æ–­å¼€ç°æœ‰è¿æ¥
                    connections.disconnect("default")
                    logger.info("Disconnected existing Milvus connection")
                except Exception as e:
                    logger.warning(f"Failed to disconnect existing connection: {e}")
            
            # å»ºç«‹æ–°è¿æ¥
            connections.connect(
                alias="default",
                host=host,
                port=str(port)
            )
            
            # éªŒè¯è¿æ¥
            if not connections.has_connection("default"):
                raise Exception("Failed to establish Milvus connection")
            
            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…ˆåŠ è½½æ¨¡å‹å†åˆå§‹åŒ–é›†åˆï¼‰
            if not self.model:
                self.model = self._load_embedding_model(model_name)
                if not self.model:
                    logger.warning("âš ï¸ æœªèƒ½åŠ è½½åµŒå…¥æ¨¡å‹ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡")
                    self.dimension = 384  # ä½¿ç”¨é»˜è®¤ç»´åº¦
                else:
                    # æ£€æµ‹æ¨¡å‹ç»´åº¦
                    self.dimension = self._detect_model_dimension(self.model)
                    logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.dimension}")
            
            # æ™ºèƒ½åˆå§‹åŒ–é›†åˆï¼ˆå¤„ç†ç»´åº¦åŒ¹é…ï¼‰
            collection_initialized = self._initialize_collection_smart()
            
            if collection_initialized:
                logger.info("âœ… Vector service initialized successfully")
            else:
                logger.error("âŒ Collection initialization failed")
                return False
                    
            self.is_available = True
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize vector service: {e}")
            self.is_available = False
            return False
    
    def _initialize_collection_smart(self):
        """æ™ºèƒ½åˆå§‹åŒ–Milvusé›†åˆï¼Œå¤„ç†ç»´åº¦åŒ¹é…é—®é¢˜"""
        try:
            from pymilvus import utility
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if utility.has_collection(self.collection_name):
                # è·å–ç°æœ‰é›†åˆçš„ç»´åº¦
                existing_dim = self._get_existing_collection_dimension()
                
                if existing_dim is None:
                    logger.error("âš ï¸ æ— æ³•è·å–ç°æœ‰é›†åˆçš„ç»´åº¦ä¿¡æ¯")
                    return False
                
                # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                if existing_dim == self.dimension:
                    logger.info(f"âœ… é›†åˆ '{self.collection_name}' å·²å­˜åœ¨ï¼Œç»´åº¦åŒ¹é…: {existing_dim}")
                    return True
                else:
                    # ç»´åº¦ä¸åŒ¹é…ï¼Œå¤„ç†å†²çª
                    logger.warning(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…: ç°æœ‰é›†åˆ={existing_dim}, æ¨¡å‹={self.dimension}")
                    return self._handle_dimension_mismatch(self.dimension, existing_dim)
            else:
                # é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°é›†åˆ
                logger.info(f"ğŸ“¦ é›†åˆ '{self.collection_name}' ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°é›†åˆ...")
                return self._create_collection_with_dimension(self.dimension)
            
        except Exception as e:
            logger.error(f"Failed to smart initialize collection: {e}")
            return False
    
    def _initialize_collection(self):
        """åˆå§‹åŒ–Milvusé›†åˆï¼ˆä¿ç•™åŸæ–¹æ³•ä»¥å…¼å®¹æ€§ï¼‰"""
        try:
            from pymilvus import Collection, FieldSchema, CollectionSchema, DataType, utility
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if utility.has_collection(self.collection_name):
                logger.info(f"Collection '{self.collection_name}' already exists")
                return True
            
            # ä½¿ç”¨å½“å‰ç»´åº¦ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            dimension = self.dimension or 384
            
            # å®šä¹‰å­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ]
            
            # åˆ›å»ºé›†åˆæ¶æ„
            schema = CollectionSchema(fields, f"Document vectors collection")
            
            # åˆ›å»ºé›†åˆ
            collection = Collection(self.collection_name, schema)
            
            # åˆ›å»ºç´¢å¼•
            index_params = {
                "metric_type": "COSINE",
                "index_type": "IVF_FLAT",
                "params": {"nlist": 128}
            }
            
            collection.create_index("vector", index_params)
            logger.info(f"âœ… Created collection '{self.collection_name}' with index")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize collection: {e}")
            return False
    
    def encode_text(self, text: str) -> Optional[List[float]]:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        global _model_instance
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹å®ä¾‹
        current_model = self.model or _model_instance
        
        # ç¡®ä¿dimensionæœ‰å€¼
        dimension = self.dimension or 384
        
        if not self.is_available or not current_model:
            logger.warning(f"Text encoding skipped - service available: {self.is_available}, model loaded: {current_model is not None}")
            # è¿”å›æ¨¡æ‹Ÿå‘é‡
            return [0.0] * dimension
            
        try:
            logger.debug(f"Encoding text with model: {type(current_model).__name__}")
            # ä½¿ç”¨sentence-transformersç¼–ç æ–‡æœ¬
            vector = current_model.encode(text, normalize_embeddings=True)
            result_vector = vector.tolist()
            
            # éªŒè¯å‘é‡ç»´åº¦æ˜¯å¦æ­£ç¡®
            actual_dim = len(result_vector)
            if actual_dim != dimension:
                logger.warning(f"âš ï¸ å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ›={dimension}, å®é™…={actual_dim}")
                # æ›´æ–°ç»´åº¦è®¾ç½®
                self.dimension = actual_dim
                logger.info(f"ğŸ”„ å·²æ›´æ–°dimensionä¸º: {actual_dim}")
            
            # éªŒè¯å‘é‡æ˜¯å¦ä¸ºé›¶å‘é‡
            vector_sum = sum(abs(v) for v in result_vector)
            if vector_sum < 1e-6:
                logger.error(f"Generated zero vector! Text: {text[:50]}...")
                logger.error(f"Model type: {type(current_model)}")
                logger.error(f"Vector shape: {vector.shape if hasattr(vector, 'shape') else 'unknown'}")
            else:
                logger.debug(f"âœ… Generated valid vector, dim: {actual_dim}, sum: {vector_sum:.6f}, range: [{min(result_vector):.4f}, {max(result_vector):.4f}]")
            
            return result_vector
        except Exception as e:
            logger.error(f"Failed to encode text: {e}")
            logger.error(f"Model instance: {current_model}")
            logger.error(f"Text sample: {text[:100]}...")
            return [0.0] * dimension
    
    def insert_vectors(self, vectors_data: List[Dict[str, Any]]) -> bool:
        """æ’å…¥å‘é‡æ•°æ®åˆ°Milvus"""
        if not self.is_available:
            logger.info(f"Vector insertion skipped for {len(vectors_data)} items - service not available")
            return True
            
        try:
            from pymilvus import Collection
            
            collection = Collection(self.collection_name)
            
            # ç›´æ¥ä½¿ç”¨å­—å…¸åˆ—è¡¨æ ¼å¼ - pymilvus 2.5.x çš„æ­£ç¡®æ ¼å¼
            result = collection.insert(vectors_data)
            collection.flush()
            
            logger.info(f"âœ… Inserted {len(vectors_data)} vectors into collection")
            return True
            
        except Exception as e:
            logger.error(f"Failed to insert vectors: {e}")
            return False
    
    def search_similar(self, query_text: str, top_k: int = 10, document_id: str = None, min_score: float = 0.0) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        if not self.is_available:
            logger.info("Vector search skipped - service not available")
            return []
            
        try:
            from pymilvus import Collection
            
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
            query_vector = self.encode_text(query_text)
            if not query_vector:
                return []
            
            collection = Collection(self.collection_name)
            collection.load()
            
            # æœç´¢å‚æ•°
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": 10}
            }
            
            # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼
            expr = None
            if document_id:
                expr = f'document_id == "{document_id}"'
            
            # æ‰§è¡Œæœç´¢
            results = collection.search(
                data=[query_vector],
                anns_field="vector",
                param=search_params,
                limit=top_k,
                expr=expr,
                output_fields=["document_id", "chunk_id", "text"]
            )
            
            # å¤„ç†ç»“æœï¼Œæ·»åŠ ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            similar_docs = []
            for hits in results:
                for hit in hits:
                    score = float(hit.score)
                    # åªè¿”å›ç›¸ä¼¼åº¦å¤§äºç­‰äºé˜ˆå€¼çš„ç»“æœ
                    if score >= min_score:
                        similar_docs.append({
                            "document_id": hit.entity.get("document_id"),
                            "chunk_id": hit.entity.get("chunk_id"), 
                            "text": hit.entity.get("text"),
                            "score": score
                        })
            
            logger.info(f"Found {len(similar_docs)} similar documents (filtered by min_score={min_score})")
            return similar_docs
            
        except Exception as e:
            logger.error(f"Failed to search similar documents: {e}")
            return []

    def search_by_keywords(self, query_text: str, top_k: int = 10, document_id: str = None) -> List[Dict[str, Any]]:
        """åŸºäºå…³é”®è¯çš„æ–‡æœ¬æœç´¢ï¼ˆè¡¥å……è¯­ä¹‰æœç´¢ï¼‰"""
        if not self.is_available:
            logger.info("Keyword search skipped - service not available")
            return []
            
        try:
            from pymilvus import Collection
            
            collection = Collection(self.collection_name)
            collection.load()
            
            # æå–æŸ¥è¯¢å…³é”®è¯
            keywords = self._extract_query_keywords(query_text)
            if not keywords:
                return []
            
            # æ„å»ºå…³é”®è¯åŒ¹é…è¡¨è¾¾å¼
            keyword_expressions = []
            for keyword in keywords:
                # ä½¿ç”¨LIKEæ“ä½œç¬¦è¿›è¡Œæ–‡æœ¬åŒ¹é…
                keyword_expressions.append(f'text like "%{keyword}%"')
            
            # ç»„åˆè¡¨è¾¾å¼
            if len(keyword_expressions) == 1:
                expr = keyword_expressions[0]
            else:
                # ä½¿ç”¨ORè¿æ¥ï¼Œåªè¦åŒ…å«ä»»ä¸€å…³é”®è¯å³å¯
                expr = f"({' or '.join(keyword_expressions)})"
            
            # å¦‚æœæŒ‡å®šäº†æ–‡æ¡£IDï¼Œæ·»åŠ è¿‡æ»¤æ¡ä»¶
            if document_id:
                expr = f'({expr}) and document_id == "{document_id}"'
            
            logger.info(f"å…³é”®è¯æœç´¢è¡¨è¾¾å¼: {expr}")
            
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨queryè€Œä¸æ˜¯searchï¼Œå› ä¸ºæˆ‘ä»¬è¦åŸºäºæ–‡æœ¬åŒ¹é…ï¼‰
            results = collection.query(
                expr=expr,
                output_fields=["document_id", "chunk_id", "text"],
                limit=top_k
            )
            
            # å¤„ç†ç»“æœï¼Œè®¡ç®—åŒ¹é…åº¦
            keyword_docs = []
            for result in results:
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                text = result.get("text", "")
                match_score = self._calculate_keyword_match_score(text, keywords)
                
                keyword_docs.append({
                    "document_id": result.get("document_id"),
                    "chunk_id": result.get("chunk_id"),
                    "text": text,
                    "score": match_score,
                    "matched_keywords": self._find_matched_keywords(text, keywords)
                })
            
            # æŒ‰åŒ¹é…åº¦æ’åº
            keyword_docs.sort(key=lambda x: x["score"], reverse=True)
            
            logger.info(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(keyword_docs)} ä¸ªåŒ¹é…ç»“æœ")
            return keyword_docs[:top_k]
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_query_keywords(self, query_text: str) -> List[str]:
        """ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        import re
        
        keywords = []
        
        # 1. æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2ä¸ªæˆ–ä»¥ä¸Šè¿ç»­æ±‰å­—ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', query_text)
        keywords.extend(chinese_words)
        
        # 2. æå–è‹±æ–‡å•è¯ï¼ˆ2ä¸ªæˆ–ä»¥ä¸Šå­—ç¬¦ï¼‰
        english_words = re.findall(r'[a-zA-Z]{2,}', query_text)
        keywords.extend(english_words)
        
        # 3. æå–æ•°å­—æ¨¡å¼
        numbers = re.findall(r'\d{2,}', query_text)
        keywords.extend(numbers)
        
        # 4. æå–ä¸“ä¸šæœ¯è¯­ï¼ˆåŒ…å«å¤§å°å†™å­—æ¯å’Œæ•°å­—çš„ç»„åˆï¼‰
        technical_terms = re.findall(r'[a-zA-Z]+\d+|[A-Z]{2,}', query_text)
        keywords.extend(technical_terms)
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_keywords = list(set(keywords))
        # è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯æ±‡
        filtered_keywords = [kw for kw in unique_keywords if len(kw) >= 2]
        
        logger.info(f"æå–çš„å…³é”®è¯: {filtered_keywords}")
        return filtered_keywords
    
    def _calculate_keyword_match_score(self, text: str, keywords: List[str]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åº¦åˆ†æ•°"""
        if not text or not keywords:
            return 0.0
        
        text_lower = text.lower()
        total_matches = 0
        unique_matches = 0
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            matches = text_lower.count(keyword_lower)
            if matches > 0:
                total_matches += matches
                unique_matches += 1
        
        # è®¡ç®—åˆ†æ•°ï¼šè€ƒè™‘åŒ¹é…çš„å…³é”®è¯æ•°é‡å’Œæ€»åŒ¹é…æ¬¡æ•°
        keyword_coverage = unique_matches / len(keywords)  # å…³é”®è¯è¦†ç›–ç‡
        match_density = min(total_matches / len(text.split()), 1.0)  # åŒ¹é…å¯†åº¦
        
        # ç»¼åˆåˆ†æ•°
        score = keyword_coverage * 0.7 + match_density * 0.3
        return min(score, 1.0)
    
    def _find_matched_keywords(self, text: str, keywords: List[str]) -> List[str]:
        """æ‰¾åˆ°æ–‡æœ¬ä¸­åŒ¹é…çš„å…³é”®è¯"""
        text_lower = text.lower()
        matched = []
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                matched.append(keyword)
        
        return matched

    def delete_vectors(self, document_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„å‘é‡"""
        if not self.is_available:
            logger.info(f"Vector deletion skipped for document {document_id} - service not available")
            return True
            
        try:
            from pymilvus import Collection
            
            collection = Collection(self.collection_name)
            
            # åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡
            expr = f'document_id == "{document_id}"'
            collection.delete(expr)
            collection.flush()
            
            logger.info(f"âœ… Deleted vectors for document {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete vectors for document {document_id}: {e}")
            return False

    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_available:
            logger.info("Collection stats retrieval skipped - service not available")
            return {
                "total_entities": 0,
                "index_type": "unknown",
                "metric_type": "unknown",
                "collection_name": self.collection_name,
                "status": "unavailable"
            }
            
        try:
            from pymilvus import Collection, utility
            
            if not utility.has_collection(self.collection_name):
                return {
                    "total_entities": 0,
                    "index_type": "none",
                    "metric_type": "none",
                    "collection_name": self.collection_name,
                    "status": "not_created"
                }
            
            collection = Collection(self.collection_name)
            collection.load()
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "total_entities": collection.num_entities,
                "collection_name": self.collection_name,
                "status": "loaded"
            }
            
            # å°è¯•è·å–ç´¢å¼•ä¿¡æ¯
            try:
                indexes = collection.indexes
                if indexes:
                    index_info = indexes[0]
                    stats["index_type"] = index_info.params.get("index_type", "unknown")
                    stats["metric_type"] = index_info.params.get("metric_type", "unknown")
                else:
                    stats["index_type"] = "none"
                    stats["metric_type"] = "none"
            except:
                stats["index_type"] = "unknown"
                stats["metric_type"] = "unknown"
            
            logger.info(f"Collection stats: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get collection stats: {e}")
            return {
                "total_entities": 0,
                "index_type": "error",
                "metric_type": "error",
                "collection_name": self.collection_name,
                "status": "error",
                "error": str(e)
            }
    
    def generate_vectors_data(self, document_id: str, text_chunks: List[str], 
                             enhanced_chunks: List[str] = None) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå‘é‡æ•°æ®
        
        Args:
            document_id: æ–‡æ¡£ID
            text_chunks: åŸå§‹æ–‡æœ¬å—ï¼ˆç”¨äºå­˜å‚¨å’Œæ˜¾ç¤ºï¼‰
            enhanced_chunks: å¢å¼ºæ–‡æœ¬å—ï¼ˆç”¨äºå‘é‡åŒ–ï¼ŒåŒ…å«é¢å¤–ä¸Šä¸‹æ–‡ï¼‰
        """
        vectors_data = []
        
        # å¦‚æœæ²¡æœ‰æä¾›å¢å¼ºæ–‡æœ¬å—ï¼Œåˆ™ä½¿ç”¨åŸå§‹æ–‡æœ¬å—
        if enhanced_chunks is None:
            enhanced_chunks = text_chunks
        
        for i, (original_chunk, enhanced_chunk) in enumerate(zip(text_chunks, enhanced_chunks)):
            if not original_chunk.strip():
                continue
                
            chunk_id = f"chunk_{i}_{str(uuid.uuid4())[:8]}"
            # ä½¿ç”¨å¢å¼ºæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
            vector = self.encode_text(enhanced_chunk.strip())
            
            if vector:
                vectors_data.append({
                    'document_id': str(document_id),
                    'chunk_id': chunk_id,
                    'text': original_chunk.strip(),  # å­˜å‚¨åŸå§‹æ–‡æœ¬
                    'vector': vector  # ä½†å‘é‡åŸºäºå¢å¼ºæ–‡æœ¬
                })
        
        return vectors_data
    
    @abstractmethod
    def extract_text(self, file_path: str) -> Dict[str, Any]:
        """
        æå–æ–‡ä»¶å†…å®¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            DictåŒ…å«:
            - success: bool, æ˜¯å¦æˆåŠŸ
            - text: str, æå–çš„æ–‡æœ¬å†…å®¹
            - metadata: dict, æ–‡ä»¶å…ƒæ•°æ®
            - chunks: list, æ–‡æœ¬åˆ†å—
            - error: str, é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        pass
    
    @abstractmethod
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        æ–‡æœ¬åˆ†å—
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            chunk_size: å—å¤§å°
            overlap: é‡å å¤§å°
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def get_supported_extensions(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        
        Returns:
            æ”¯æŒçš„æ‰©å±•ååˆ—è¡¨
        """
        pass
    
    def chunk_text_smart(self, text: str, chunk_size: int = 800, overlap: int = 150) -> List[str]:
        """æ™ºèƒ½åˆ†æ®µï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        sentences = self._split_sentences(text)
        current_chunk = ""
        
        for sentence in sentences:
            # å¦‚æœæ·»åŠ è¿™ä¸ªå¥å­ä¸ä¼šè¶…è¿‡é™åˆ¶
            if len(current_chunk + sentence) <= chunk_size:
                current_chunk += sentence
            else:
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # å¼€å§‹æ–°chunkï¼ŒåŒ…å«overlap
                if chunks and overlap > 0:
                    overlap_text = current_chunk[-overlap:]
                    current_chunk = overlap_text + sentence
                else:
                    current_chunk = sentence
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        """æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬"""
        import re
        # ä¸­æ–‡å¥å·ã€è‹±æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]\s*', text)
        # ä¸ºåˆ†å‰²åçš„å¥å­æ·»åŠ å¥å·ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
        result = []
        for sentence in sentences:
            if sentence.strip():
                # æ£€æŸ¥åŸå¥å­æ˜¯å¦ä»¥æ ‡ç‚¹ç»“å°¾
                if sentence.strip() and not re.search(r'[ã€‚ï¼ï¼Ÿ.!?]$', sentence.strip()):
                    result.append(sentence.strip() + 'ã€‚')
                else:
                    result.append(sentence.strip())
        return result
    
    def vectorize_document_enhanced(self, file_path: str, document_id: str, chunk_size: int = 800, overlap: int = 150) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆæ–‡æ¡£å‘é‡åŒ–ï¼ŒåŒ…å«å…³é”®è¯ç´¢å¼•"""
        try:
            # 1. æå–æ–‡æœ¬å†…å®¹
            extract_result = self.extract_text(file_path)
            if not extract_result['success']:
                return extract_result
            
            text = extract_result['text']
            
            # 2. æå–å…³é”®è¯
            keywords = self._extract_keywords_from_content(text, os.path.basename(file_path))
            
            # 3. æ™ºèƒ½åˆ†æ®µ
            chunks = self.chunk_text_smart(text, chunk_size, overlap)
            
            # 4. ä¸ºæ¯ä¸ªchunkæ·»åŠ å…³é”®è¯å¢å¼º
            enhanced_chunks = []
            for i, chunk in enumerate(chunks):
                # æ‰¾åˆ°ä¸æ­¤chunkç›¸å…³çš„å…³é”®è¯
                chunk_keywords = [kw for kw in keywords if kw.lower() in chunk.lower()]
                
                # åˆ›å»ºå¢å¼ºæ–‡æœ¬ï¼ˆåŸæ–‡æœ¬ + å…³é”®è¯ï¼‰
                enhanced_text = chunk
                if chunk_keywords:
                    enhanced_text += f"\nå…³é”®è¯: {', '.join(chunk_keywords)}"
                
                enhanced_chunks.append({
                    'text': chunk,
                    'enhanced_text': enhanced_text,
                    'keywords': chunk_keywords,
                    'chunk_index': i
                })
            
            # 5. å‘é‡åŒ–å¢å¼ºæ–‡æœ¬
            vectors_data = []
            for chunk_data in enhanced_chunks:
                chunk_id = f"chunk_{chunk_data['chunk_index']}_{str(uuid.uuid4())[:8]}"
                vector = self.encode_text(chunk_data['enhanced_text'])
                
                if vector:
                    vectors_data.append({
                        'document_id': str(document_id),
                        'chunk_id': chunk_id,
                        'text': chunk_data['text'],  # å­˜å‚¨åŸå§‹æ–‡æœ¬
                        'vector': vector
                    })
            
            # 6. æ’å…¥å‘é‡æ•°æ®åº“
            insert_success = self.insert_vectors(vectors_data)
            
            return {
                'success': True,
                'text': text,
                'metadata': extract_result['metadata'],
                'chunks': [{'id': f'chunk_{i}', 'content': chunk['text']} for i, chunk in enumerate(enhanced_chunks)],
                'vectors_count': len(vectors_data),
                'vector_insert_success': insert_success,
                'keywords': keywords,
                'chunk_size': chunk_size,
                'overlap': overlap,
                'enhancement': 'enabled'
            }
            
        except Exception as e:
            logger.error(f"å¢å¼ºå‘é‡åŒ–å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _extract_keywords_from_content(self, text: str, filename: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–å…³é”®è¯ç”¨äºæ£€ç´¢ä¼˜åŒ–"""
        keywords = []
        
        # æ–‡ä»¶åå…³é”®è¯
        base_name = os.path.splitext(filename)[0]
        filename_words = base_name.replace('_', ' ').replace('-', ' ').split()
        keywords.extend([word for word in filename_words if len(word) > 1])
        
        # æ–‡æœ¬å†…å®¹å…³é”®è¯
        if text:
            import re
            # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2ä¸ªæˆ–ä»¥ä¸Šè¿ç»­æ±‰å­—ï¼‰
            chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', text)
            keywords.extend(chinese_words[:50])  # é™åˆ¶æ•°é‡
            
            # æå–è‹±æ–‡å•è¯ï¼ˆ2ä¸ªæˆ–ä»¥ä¸Šå­—ç¬¦ï¼‰
            english_words = re.findall(r'[a-zA-Z]{2,}', text)
            keywords.extend(english_words[:50])  # é™åˆ¶æ•°é‡
            
            # æå–æ•°å­—æ¨¡å¼
            numbers = re.findall(r'\d{2,}', text)
            keywords.extend(numbers[:20])  # é™åˆ¶æ•°é‡
            
            # æå–å¯èƒ½çš„ä¸“ä¸šæœ¯è¯­
            technical_terms = re.findall(r'[A-Z]{2,}|[a-zA-Z]+\d+', text)
            keywords.extend(technical_terms[:30])
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_keywords = list(set(keywords))
        return [kw for kw in unique_keywords if len(kw) > 1 and not kw.isspace()][:100]  # é™åˆ¶æ€»æ•°

    def vectorize_document(self, file_path: str, document_id: str, chunk_size: int = 1000, overlap: int = 200) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ–‡æ¡£å‘é‡åŒ–æµç¨‹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ID
            chunk_size: åˆ†å—å¤§å°
            overlap: é‡å å¤§å°
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # 1. æå–æ–‡æœ¬
            extract_result = self.extract_text(file_path)
            if not extract_result['success']:
                return extract_result
            
            # 2. æ–‡æœ¬åˆ†å—
            if chunk_size != 1000 or overlap != 200:
                # é‡æ–°åˆ†å—
                chunks = self.chunk_text(extract_result['text'], chunk_size, overlap)
            else:
                chunks = extract_result.get('chunks', [])
            
            # 3. ç”Ÿæˆå‘é‡æ•°æ®
            vectors_data = self.generate_vectors_data(document_id, chunks)
            
            # 4. æ’å…¥å‘é‡æ•°æ®åº“
            insert_success = self.insert_vectors(vectors_data)
            
            return {
                'success': True,
                'text': extract_result['text'],
                'metadata': extract_result['metadata'],
                'chunks': [{'id': f'chunk_{i}', 'content': chunk} for i, chunk in enumerate(chunks)],
                'vectors_count': len(vectors_data),
                'vector_insert_success': insert_success,
                'chunk_size': chunk_size,
                'overlap': overlap
            }
            
        except Exception as e:
            logger.error(f"Document vectorization failed: {e}")
            return {
                'success': False,
                'error': str(e)
            } 