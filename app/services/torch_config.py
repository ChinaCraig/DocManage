"""
Torché…ç½®æ¨¡å—
ä¸“é—¨ç”¨äºè§£å†³torch DataLoader pin_memoryè­¦å‘Šé—®é¢˜
"""
import os
import warnings
import logging

logger = logging.getLogger(__name__)

def configure_torch_for_cpu_gpu_compatibility():
    """é…ç½®torchä»¥ç¡®ä¿CPUå’ŒGPUç¯å¢ƒéƒ½èƒ½æ­£å¸¸å·¥ä½œ"""
    try:
        import torch
        
        # è®¾ç½®åŸºæœ¬ç¯å¢ƒå˜é‡
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§ï¼ˆåŒ…æ‹¬CUDAå’ŒMPSï¼‰
        has_cuda = torch.cuda.is_available()
        has_mps = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        has_gpu = has_cuda or has_mps
        
        if not has_gpu:
            # CPUç¯å¢ƒé…ç½®
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            os.environ['PYTORCH_DISABLE_PIN_MEMORY'] = '1'
            
            # ä¿®è¡¥torch DataLoaderä»¥ç¦ç”¨pin_memory
            _patch_dataloader_pin_memory()
            
            logger.info("ğŸ”§ Configured torch for CPU-only environment")
        else:
            # GPUç¯å¢ƒé…ç½®
            if has_cuda:
                logger.info("ğŸ”§ Configured torch for CUDA GPU environment")
            elif has_mps:
                logger.info("ğŸ”§ Configured torch for MPS (Metal) GPU environment")
                # ä¸ºMPSè®¾ç½®ç‰¹å®šé…ç½®
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            
        # é€šç”¨è®¾ç½®
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        # æŠ‘åˆ¶ç›¸å…³è­¦å‘Š
        warnings.filterwarnings('ignore', message='.*pin_memory.*')
        warnings.filterwarnings('ignore', message='.*device pinned memory.*')
        
        return True
        
    except ImportError:
        logger.warning("torch not available, skipping torch configuration")
        return False
    except Exception as e:
        logger.error(f"Failed to configure torch: {e}")
        return False

def _patch_dataloader_pin_memory():
    """ä¿®è¡¥DataLoaderä»¥ç¦ç”¨pin_memory"""
    try:
        import torch
        from torch.utils.data import DataLoader
        
        # ä¿å­˜åŸå§‹__init__æ–¹æ³•
        original_init = DataLoader.__init__
        
        def patched_init(self, *args, **kwargs):
            # åœ¨éCUDAç¯å¢ƒä¸­å¼ºåˆ¶ç¦ç”¨pin_memory
            if not torch.cuda.is_available():
                kwargs['pin_memory'] = False
            return original_init(self, *args, **kwargs)
        
        # åº”ç”¨è¡¥ä¸
        DataLoader.__init__ = patched_init
        
        logger.info("ğŸ”§ Patched DataLoader to disable pin_memory on CPU")
        
    except Exception as e:
        logger.error(f"Failed to patch DataLoader: {e}")

def setup_sentence_transformers_environment():
    """ä¸“é—¨ä¸ºsentence-transformersè®¾ç½®ç¯å¢ƒ"""
    try:
        # åœ¨å¯¼å…¥sentence-transformersä¹‹å‰è®¾ç½®ç¯å¢ƒ
        configure_torch_for_cpu_gpu_compatibility()
        
        # è®¾ç½®sentence-transformersç‰¹å®šçš„ç¯å¢ƒå˜é‡
        os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.expanduser('~/.cache/sentence_transformers')
        
        # æŠ‘åˆ¶sentence-transformersç›¸å…³è­¦å‘Š
        warnings.filterwarnings('ignore', category=UserWarning, module='sentence_transformers')
        
        logger.info("âœ… sentence-transformers environment configured")
        
    except Exception as e:
        logger.error(f"Failed to setup sentence-transformers environment: {e}")

# åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨é…ç½®
configure_torch_for_cpu_gpu_compatibility() 