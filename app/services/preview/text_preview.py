from .base_preview import BasePreviewService
import os
import logging
from datetime import datetime
import chardet
import mimetypes
import re

logger = logging.getLogger(__name__)

class TextPreviewService(BasePreviewService):
    """æ–‡æœ¬æ–‡ä»¶é¢„è§ˆæœåŠ¡"""
    
    def __init__(self):
        super().__init__()
        self.supported_formats = ['txt', 'text', 'log', 'md', 'markdown', 'csv', 'json', 'xml', 'py', 'js', 'html', 'css', 'sql']
        self.max_content_length = 50000  # æœ€å¤§å†…å®¹é•¿åº¦ï¼Œé¿å…å†…å­˜é—®é¢˜
    
    def extract_content(self, file_path, document_id=None):
        """æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯MCPåˆ›å»ºçš„è™šæ‹Ÿæ–‡ä»¶
        if file_path.startswith('mcp_created/'):
            return self._extract_mcp_content(file_path, document_id)
        
        if not self.validate_file(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è¯»å–: {file_path}")
        
        try:
            # æ£€æµ‹æ–‡ä»¶ç¼–ç 
            encoding = self._detect_encoding(file_path)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding=encoding, errors='ignore') as file:
                content = file.read()
            
            # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªæ–­å¹¶æ·»åŠ æç¤º
            if len(content) > self.max_content_length:
                content = content[:self.max_content_length] + "\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·ä¸‹è½½æ–‡ä»¶æŸ¥çœ‹) ..."
                is_truncated = True
            else:
                is_truncated = False
            
            # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
            lines = content.count('\n') + 1 if content else 0
            words = len(content.split()) if content else 0
            chars = len(content)
            
            content_data = {
                'type': 'text',
                'content': content,
                'encoding': encoding,
                'is_truncated': is_truncated,
                'metadata': {
                    'lines': lines,
                    'words': words,
                    'characters': chars,
                    'file_type': self._get_file_type_description(file_path)
                }
            }
            
            logger.info(f"âœ… æ–‡æœ¬æ–‡ä»¶å†…å®¹æå–æˆåŠŸ: {file_path}, ç¼–ç : {encoding}")
            return content_data
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬æ–‡ä»¶å†…å®¹æå–å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            raise
    
    def _extract_mcp_content(self, file_path, document_id):
        """æå–MCPåˆ›å»ºçš„æ–‡ä»¶å†…å®¹"""
        try:
            logger.info(f"ğŸ” å¤„ç†MCPæ–‡ä»¶: {file_path}")
            
            # å¯¼å…¥æ¨¡å‹
            from app.models.document_models import DocumentNode, DocumentContent
            from app import db
            
            if document_id:
                # ä½¿ç”¨document_idæŸ¥æ‰¾
                document = db.session.query(DocumentNode).filter_by(
                    id=document_id,
                    is_deleted=False
                ).first()
            else:
                # ä½¿ç”¨file_pathæŸ¥æ‰¾
                document = db.session.query(DocumentNode).filter_by(
                    file_path=file_path,
                    is_deleted=False
                ).first()
                
            if not document:
                raise FileNotFoundError(f"MCPæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # æ£€æŸ¥è™šæ‹Ÿæ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿä¸­
            import os
            full_virtual_path = os.path.join('uploads', file_path)
            content = ""
            
            if os.path.exists(full_virtual_path):
                # æ–‡ä»¶å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå°è¯•è¯»å–
                logger.info(f"ğŸ“ è™šæ‹Ÿæ–‡ä»¶å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿ: {full_virtual_path}")
                
                file_ext = document.file_type.lower()
                if file_ext in ['pdf', 'xlsx', 'docx']:
                    # å¯¹äºäºŒè¿›åˆ¶æ ¼å¼ï¼Œä»æ•°æ®åº“è·å–æ–‡æœ¬å†…å®¹
                    content_record = db.session.query(DocumentContent).filter_by(
                        document_id=document.id
                    ).first()
                    
                    if content_record and content_record.content_text:
                        content = content_record.content_text
                    else:
                        content = f"[{file_ext.upper()}æ–‡æ¡£] åŸå§‹å†…å®¹è¯·ä¸‹è½½æ–‡ä»¶æŸ¥çœ‹"
                        
                    # æ·»åŠ ä¸‹è½½é“¾æ¥ä¿¡æ¯
                    content += f"\n\nğŸ’¾ æ–‡ä»¶å·²ç”Ÿæˆä¸º {file_ext.upper()} æ ¼å¼ï¼Œå¯é€šè¿‡ä¸‹è½½åŠŸèƒ½è·å–å®Œæ•´æ–‡æ¡£ã€‚"
                    
                else:
                    # æ–‡æœ¬æ ¼å¼ï¼Œç›´æ¥è¯»å–æ–‡ä»¶
                    try:
                        with open(full_virtual_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                    except UnicodeDecodeError:
                        # å°è¯•å…¶ä»–ç¼–ç 
                        for encoding in ['gbk', 'gb2312', 'latin1']:
                            try:
                                with open(full_virtual_path, 'r', encoding=encoding) as f:
                                    content = f.read()
                                break
                            except:
                                continue
            else:
                # æ–‡ä»¶ä¸å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œä»æ•°æ®åº“è¯»å–
                logger.info(f"ğŸ’¾ ä»æ•°æ®åº“è¯»å–MCPæ–‡ä»¶å†…å®¹: {file_path}")
                
                content_record = db.session.query(DocumentContent).filter_by(
                    document_id=document.id
                ).first()
                
                if content_record and content_record.content_text:
                    content = content_record.content_text
                else:
                    content = ""
            
            # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªæ–­å¹¶æ·»åŠ æç¤º
            if len(content) > self.max_content_length:
                content = content[:self.max_content_length] + "\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·ä¸‹è½½æ–‡ä»¶æŸ¥çœ‹) ..."
                is_truncated = True
            else:
                is_truncated = False
            
            # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
            lines = content.count('\n') + 1 if content else 0
            words = len(content.split()) if content else 0
            chars = len(content)
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è®¾ç½®æè¿°
            file_type_desc = {
                'txt': 'æ–‡æœ¬æ–‡æ¡£',
                'pdf': 'PDFæ–‡æ¡£',
                'xlsx': 'Excelå·¥ä½œç°¿',
                'docx': 'Wordæ–‡æ¡£'
            }.get(document.file_type, 'ç”Ÿæˆæ–‡æ¡£')
            
            content_data = {
                'type': 'text',
                'content': content,
                'encoding': 'utf-8',
                'is_truncated': is_truncated,
                'metadata': {
                    'lines': lines,
                    'words': words,
                    'characters': chars,
                    'file_type': file_type_desc,
                    'source': 'mcp_created',
                    'format': document.file_type,
                    'has_binary_file': os.path.exists(full_virtual_path),
                    'mime_type': document.mime_type
                }
            }
            
            logger.info(f"âœ… MCPæ–‡ä»¶å†…å®¹æå–æˆåŠŸ: {file_path}, æ ¼å¼: {document.file_type}, å­—ç¬¦æ•°: {chars}")
            return content_data
            
        except Exception as e:
            logger.error(f"âŒ MCPæ–‡ä»¶å†…å®¹æå–å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            raise
    
    def _detect_encoding(self, file_path):
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            # è¯»å–æ–‡ä»¶çš„å‰å‡ ä¸ªå­—èŠ‚æ¥æ£€æµ‹ç¼–ç 
            with open(file_path, 'rb') as file:
                raw_data = file.read(10000)  # è¯»å–å‰10KB
            
            # ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
            detected = chardet.detect(raw_data)
            encoding = detected.get('encoding', 'utf-8')
            confidence = detected.get('confidence', 0)
            
            # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œä½¿ç”¨å¸¸è§ç¼–ç 
            if confidence < 0.7:
                # å°è¯•å¸¸è§çš„ä¸­æ–‡ç¼–ç 
                for enc in ['utf-8', 'gbk', 'gb2312', 'big5', 'utf-16']:
                    try:
                        with open(file_path, 'r', encoding=enc) as f:
                            f.read(1000)  # å°è¯•è¯»å–ä¸€å°éƒ¨åˆ†
                        encoding = enc
                        break
                    except (UnicodeDecodeError, UnicodeError):
                        continue
            
            # ç¡®ä¿ä½¿ç”¨å®‰å…¨çš„ç¼–ç 
            if encoding is None or encoding.lower() in ['ascii']:
                encoding = 'utf-8'
            
            return encoding
            
        except Exception as e:
            logger.warning(f"ç¼–ç æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç UTF-8: {file_path}, é”™è¯¯: {str(e)}")
            return 'utf-8'
    
    def _get_file_type_description(self, file_path):
        """è·å–æ–‡ä»¶ç±»å‹æè¿°"""
        ext = os.path.splitext(file_path)[1].lower()
        
        type_descriptions = {
            '.txt': 'çº¯æ–‡æœ¬æ–‡ä»¶',
            '.log': 'æ—¥å¿—æ–‡ä»¶',
            '.md': 'Markdownæ–‡æ¡£',
            '.markdown': 'Markdownæ–‡æ¡£',
            '.csv': 'CSVæ•°æ®æ–‡ä»¶',
            '.json': 'JSONæ•°æ®æ–‡ä»¶',
            '.xml': 'XMLæ–‡æ¡£',
            '.py': 'Pythonæºä»£ç ',
            '.js': 'JavaScriptæºä»£ç ',
            '.html': 'HTMLç½‘é¡µæ–‡ä»¶',
            '.htm': 'HTMLç½‘é¡µæ–‡ä»¶',
            '.css': 'CSSæ ·å¼è¡¨',
            '.sql': 'SQLè„šæœ¬æ–‡ä»¶'
        }
        
        return type_descriptions.get(ext, 'æ–‡æœ¬æ–‡ä»¶')
    
    def get_metadata(self, file_path):
        """è·å–æ–‡æœ¬æ–‡ä»¶å…ƒæ•°æ®"""
        if not self.validate_file(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è¯»å–: {file_path}")
        
        try:
            # åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            file_stats = os.stat(file_path)
            metadata = {
                'file_size': file_stats.st_size,
                'file_size_formatted': self.format_file_size(file_stats.st_size),
                'file_type': self._get_file_type_description(file_path),
                'last_modified': datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                'created': datetime.fromtimestamp(file_stats.st_ctime).isoformat()
            }
            
            # æ£€æµ‹ç¼–ç 
            encoding = self._detect_encoding(file_path)
            metadata['encoding'] = encoding
            
            # è·å–MIMEç±»å‹
            mime_type, _ = mimetypes.guess_type(file_path)
            if mime_type:
                metadata['mime_type'] = mime_type
            
            # åˆ†ææ–‡ä»¶å†…å®¹
            try:
                with open(file_path, 'r', encoding=encoding, errors='ignore') as file:
                    content = file.read()
                
                # ç»Ÿè®¡ä¿¡æ¯
                lines = content.count('\n') + 1 if content else 0
                words = len(content.split()) if content else 0
                chars = len(content)
                chars_no_spaces = len(content.replace(' ', '').replace('\n', '').replace('\t', ''))
                
                metadata.update({
                    'lines': lines,
                    'words': words,
                    'characters': chars,
                    'characters_no_spaces': chars_no_spaces,
                    'is_empty': chars == 0,
                    'estimated_read_time': self._estimate_read_time(words)
                })
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œè¾ƒå¤š
                if lines > 1:
                    empty_lines = content.count('\n\n') + content.count('\n \n')
                    metadata['empty_lines'] = empty_lines
                    metadata['content_density'] = round((lines - empty_lines) / lines * 100, 2)
                
                # æ–‡ä»¶æ ¼å¼ç‰¹å®šåˆ†æ
                ext = os.path.splitext(file_path)[1].lower()
                if ext == '.csv':
                    metadata.update(self._analyze_csv_content(content))
                elif ext == '.json':
                    metadata.update(self._analyze_json_content(content))
                elif ext in ['.py', '.js', '.html', '.css', '.sql']:
                    metadata.update(self._analyze_code_content(content, ext))
                
            except Exception as e:
                logger.warning(f"æ— æ³•åˆ†ææ–‡ä»¶å†…å®¹: {file_path}, é”™è¯¯: {str(e)}")
                metadata['content_analysis_error'] = str(e)
            
            return metadata
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æœ¬æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            return {
                'file_size': self.get_file_size(file_path),
                'file_size_formatted': self.format_file_size(self.get_file_size(file_path)),
                'file_type': self._get_file_type_description(file_path),
                'last_modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                'error': str(e)
            }
    
    def _estimate_read_time(self, word_count):
        """ä¼°ç®—é˜…è¯»æ—¶é—´ï¼ˆæŒ‰æ¯åˆ†é’Ÿ250è¯è®¡ç®—ï¼‰"""
        if word_count == 0:
            return "0åˆ†é’Ÿ"
        
        minutes = max(1, round(word_count / 250))
        if minutes < 60:
            return f"{minutes}åˆ†é’Ÿ"
        else:
            hours = minutes // 60
            remaining_minutes = minutes % 60
            if remaining_minutes == 0:
                return f"{hours}å°æ—¶"
            else:
                return f"{hours}å°æ—¶{remaining_minutes}åˆ†é’Ÿ"
    
    def _analyze_csv_content(self, content):
        """åˆ†æCSVæ–‡ä»¶å†…å®¹"""
        try:
            lines = content.strip().split('\n')
            if not lines:
                return {}
            
            # ä¼°ç®—åˆ—æ•°ï¼ˆåŸºäºç¬¬ä¸€è¡Œï¼‰
            first_line = lines[0]
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            tab_count = first_line.count('\t')
            
            # åˆ¤æ–­åˆ†éš”ç¬¦
            if comma_count > semicolon_count and comma_count > tab_count:
                separator = ','
                columns = comma_count + 1
            elif semicolon_count > tab_count:
                separator = ';'
                columns = semicolon_count + 1
            elif tab_count > 0:
                separator = '\t'
                columns = tab_count + 1
            else:
                separator = ','
                columns = 1
            
            return {
                'csv_rows': len(lines),
                'csv_columns': columns,
                'csv_separator': separator,
                'has_header': True  # å‡è®¾æœ‰æ ‡é¢˜è¡Œ
            }
        except:
            return {}
    
    def _analyze_json_content(self, content):
        """åˆ†æJSONæ–‡ä»¶å†…å®¹"""
        try:
            import json
            data = json.loads(content)
            
            analysis = {'json_valid': True}
            
            if isinstance(data, dict):
                analysis['json_type'] = 'å¯¹è±¡'
                analysis['json_keys'] = len(data.keys())
            elif isinstance(data, list):
                analysis['json_type'] = 'æ•°ç»„'
                analysis['json_items'] = len(data)
            else:
                analysis['json_type'] = 'ç®€å•å€¼'
            
            return analysis
        except:
            return {'json_valid': False}
    
    def _analyze_code_content(self, content, extension):
        """åˆ†æä»£ç æ–‡ä»¶å†…å®¹"""
        try:
            lines = content.split('\n')
            code_lines = 0
            comment_lines = 0
            blank_lines = 0
            
            # ç®€å•çš„ä»£ç åˆ†æ
            for line in lines:
                stripped = line.strip()
                if not stripped:
                    blank_lines += 1
                elif self._is_comment_line(stripped, extension):
                    comment_lines += 1
                else:
                    code_lines += 1
            
            total_lines = len(lines)
            return {
                'code_lines': code_lines,
                'comment_lines': comment_lines,
                'blank_lines': blank_lines,
                'code_ratio': round(code_lines / total_lines * 100, 2) if total_lines > 0 else 0
            }
        except:
            return {}
    
    def _is_comment_line(self, line, extension):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ³¨é‡Šè¡Œ"""
        if extension == '.py':
            return line.startswith('#')
        elif extension == '.js':
            return line.startswith('//') or line.startswith('/*') or line.startswith('*')
        elif extension in ['.html', '.htm']:
            return '<!--' in line
        elif extension == '.css':
            return line.startswith('/*') or line.startswith('*')
        elif extension == '.sql':
            return line.startswith('--') or line.startswith('/*')
        return False
    
    def generate_thumbnail(self, file_path, output_path=None, size=(200, 200)):
        """æ–‡æœ¬æ–‡ä»¶ä¸æ”¯æŒç¼©ç•¥å›¾ç”Ÿæˆ"""
        return None
    
    def _process_hyperlinks(self, content):
        """å¤„ç†æ–‡æ¡£å†…å®¹ä¸­çš„è¶…é“¾æ¥æ ‡è®°"""
        try:
            # å¯¼å…¥æ‰€éœ€æ¨¡å—
            from app.models.document_models import DocumentNode
            from app import db
            
            # å¤„ç†[LINK:æ–‡ä»¶å:document_id]æ ¼å¼çš„è¶…é“¾æ¥
            def replace_link(match):
                file_name = match.group(1)
                doc_id = match.group(2)
                
                try:
                    # éªŒè¯æ–‡æ¡£æ˜¯å¦å­˜åœ¨
                    doc = db.session.query(DocumentNode).filter_by(
                        id=int(doc_id),
                        is_deleted=False
                    ).first()
                    
                    if doc:
                        # åˆ›å»ºå¯ç‚¹å‡»çš„HTMLé“¾æ¥
                        return f'<a href="#" class="file-name-link" onclick="selectFileFromChat({doc_id}); return false;" title="ç‚¹å‡»å®šä½å¹¶é¢„è§ˆæ–‡ä»¶">{file_name}</a>'
                    else:
                        # å¦‚æœæ–‡æ¡£ä¸å­˜åœ¨ï¼Œè¿”å›æ™®é€šæ–‡æœ¬
                        return file_name
                except (ValueError, TypeError):
                    # å¦‚æœdocument_idæ— æ•ˆï¼Œè¿”å›æ™®é€šæ–‡æœ¬
                    return file_name
            
            # åŒ¹é…[LINK:æ–‡ä»¶å:document_id]æ ¼å¼
            link_pattern = r'\[LINK:([^:]+):(\d+)\]'
            processed_content = re.sub(link_pattern, replace_link, content)
            
            # å¤„ç†[FILE:æ–‡ä»¶å:document_id]æ ¼å¼ï¼ˆä½œä¸ºå¤‡ç”¨æ ¼å¼ï¼‰
            file_pattern = r'\[FILE:([^:]+):(\d+)\]'
            processed_content = re.sub(file_pattern, replace_link, processed_content)
            
            return processed_content
            
        except Exception as e:
            logger.error(f"å¤„ç†è¶…é“¾æ¥å¤±è´¥: {e}")
            return content 